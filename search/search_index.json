{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 Hi!","title":"Home"},{"location":"#home","text":"Hi!","title":"Home"},{"location":"3D-printing-101/","text":"FDM 3D printers 101 \u00b6 Physical architecture \u00b6 How to print in any machine \u00b6 Creating the 3d model \u00b6 In order to print a 3D object, we must first have a 3D design in our hands. By the use of a CAD software, such as: * Solidworks * Inventor * Fusion 360 * FreeCAD * Many more we can create a 3D design or simply open and save one already created by another user. The latter can happen either by importing a native file into the software (i.e. for solidworks it would be a .sldprt file. This can only be opened by solidworks or any software that indicates it has the capability to do so) or a step (.stp) file, which can be opened by any CAD software. In order for a 3D design to end up translated into an STL (.stl) file first. This will be done by using an export option in a CAD software and saving an STL file to use later. STL files \u00b6 Why use STL files? This file form was firstly used for a stereolithography (SLA) printing CAD software and has since been established for most printers. It describes only the surface geometry of a three-dimensional object without any representation of color, texture or other common CAD model attributes. This is done by interpeting the surfaces as simple triangles (the parameters for these triangles can be tapered with when exporting an STL file to have more or less detail). More or less it is used because it is easier for the makers of slicing softwares & printing apps and it is simple to use it. To view an STL file, one cannot use the same CAD software that creates 3D models. But there are other available STL viewers and editors to download and use. Slicing \u00b6 Short-said, \"slicing\" means turning data from the STL file into a language that the printer can understand and translate into all its movements (g-code). The reason it's called \"slicing\" comes from the fact that a 3D print is created layer-by-layer, so what the slicer software actually does (among other things) is slicing the 3D model into thin layers. During slicing is when one can tamper with the way the final print will look. This is done through a very large amount of parameters that you can change to your liking. Usually it is better to keep the most parameters to the default settings, but there are some basic ones that you will usually want or have to edit. \u00b6 Printing parameters \u00b6 (This is a long detailed read, only if you care about quality and printing duration) \u00b6 Printing parameters are usually split into the below categories: * Printer settings * Fillament settings * Print settings Printer settings \u00b6 Depending on the printer features, the printer settings must be set so the g-code created by the slicer is made for the printer you will use. This means that there is no way the printer will try to do something it can't or shouldn't do, like trying to move beyond its boundaries or hit its nozzle on an object. The most important settings are: * Printing bed size: Defining the bed dimensions (width, depth & height) so the software knows the maximum physical size of a model the printer can handle. * Number of extruders: Most printers have a single extruder, but there are some that have two. If you use 2 extruders, you must define their relative position by giving an offset value from their center. (Dual extruders are out of the scope of this guide) * Nozzle diameter: Usually by default the nozzle diameter is 0.4mm, but it is possible to use other sizes. Note that changing the nozzle diameter setting has to be done simultaneously with changing the physical printer nozzle. Fillament settings \u00b6 These settings apply to the fillament material of the user. So, every time you try to change printing material you must change fillament settings (you can save and name settings to use them again). This may even be needed for the same material, but different brand. Typical settings to change are: * Color * Diameter: The diameter of the fillament before it enders the extruder. This typically is 1.75mm. * Extruder temperature: The temperature of the extruder must be set so that the fillament is properly melted in order to take the shape the printer gives it. The recommended value is written on the fillament package, but this is one of the most common settings to check/change when a print fails, because it is usually affected by environmental conditions. * Bed temperature: The bed temperature is set so the first layers of the print are still soft enough (but not melted) so that the print is stuck on the bed until the process finishes. This means the print will not get itself unstuck due to motion and ruin the process. The recommended value is written on the fillament package, but this is one of the most common settings to check/change when a print fails, because it is usually affected by environmental conditions. Both bed and extruder temperature can be set differently for the first and the rest layers. Print settings \u00b6 These settings can change mostly depending on the model geometry in order to properly support it. Typical settings to change are: Layer height: The layer height defines the thickness of each layer and accordingly the number of layers for the same model. This can affect the final look of the print, as thinner layers mean less ridges on the outside. But, the downside is the more layers, the longer the print.(the first layer can have different height, usually bigger, so it can stick more to the bed) Vertical and horizontal shells: The printed model has an outer surface and a filling pattern. The outer surface (or shell) can have multiple layers so it is smoother and more rigid. Infill: The infill is the way the printer completes the volume inside the outer surfaces. You can edit the density , pattern and angle of infill. Infill density defines what percentage of the volume is filled. High infill gives more rigid structures but substancially higher print times. Infill pattern controls the shape the infill has inside this volume. It can be a simple 2d pattern like squares or honeycombs same for each layer, or it can be a 3d pattern like waves or 3d honeycombs. This setting and the infill angle can affect the directional properties of a print. For instance to be able to handle more force in a certain direction. Print speed: This setting can be changed manually during print or set as a parameter during slicing. The print speed setting is modular. There can be a different setting for seperate layers, for support material, for skirts, brims, etc. So depending on your level of understanding print fails, you can change speed to correct specific problems. Support material: Creates supports for overhanging geometries. Changing support material settings can control the pattern of printing supports, how dense the layers are (meaning how easy it is to remove), etc. Skirt: Skirt is a perimeter around the print that doesn't have contact with it. It is used to make sure that the fillament is flowing correctly before beginning the print. Brim: Brim is a surface added around the first layer of the print in order to provide bigger stability for the next layers. Raft: It has the same functionality with the brim, but it extends to several layers and goes under the print. This means the first layer has only raft layers and the model layers begin after the raft has been made. Common materials \u00b6 PLA: The most common material. Cheaper, easier to print, biodegradable (debatable, though), hard and brittle. Indicative nozzle temp 190\u00b0C~205\u00b0C, indicative bed temp 55\u00b0C~65\u00b0C. ABS: Second most common material. Almost as cheap as PLA, a lot harder to print than PLA (sensitive to temperature changes; Can warp easily), acetone corrodes it, so it can be left in an acetone vat to be smoothed, hard and kind of flexible. Indicative nozzle temp 240\u00b0C~270\u00b0C, indicative bed temp 100\u00b0C~110\u00b0C. Thanks to @sspikeo.","title":"FDM 3D printers 101"},{"location":"3D-printing-101/#fdm-3d-printers-101","text":"","title":"FDM 3D printers 101"},{"location":"3D-printing-101/#physical-architecture","text":"","title":"Physical architecture"},{"location":"3D-printing-101/#how-to-print-in-any-machine","text":"","title":"How to print in any machine"},{"location":"3D-printing-101/#creating-the-3d-model","text":"In order to print a 3D object, we must first have a 3D design in our hands. By the use of a CAD software, such as: * Solidworks * Inventor * Fusion 360 * FreeCAD * Many more we can create a 3D design or simply open and save one already created by another user. The latter can happen either by importing a native file into the software (i.e. for solidworks it would be a .sldprt file. This can only be opened by solidworks or any software that indicates it has the capability to do so) or a step (.stp) file, which can be opened by any CAD software. In order for a 3D design to end up translated into an STL (.stl) file first. This will be done by using an export option in a CAD software and saving an STL file to use later.","title":"Creating the 3d model"},{"location":"3D-printing-101/#stl-files","text":"Why use STL files? This file form was firstly used for a stereolithography (SLA) printing CAD software and has since been established for most printers. It describes only the surface geometry of a three-dimensional object without any representation of color, texture or other common CAD model attributes. This is done by interpeting the surfaces as simple triangles (the parameters for these triangles can be tapered with when exporting an STL file to have more or less detail). More or less it is used because it is easier for the makers of slicing softwares & printing apps and it is simple to use it. To view an STL file, one cannot use the same CAD software that creates 3D models. But there are other available STL viewers and editors to download and use.","title":"STL files"},{"location":"3D-printing-101/#slicing","text":"Short-said, \"slicing\" means turning data from the STL file into a language that the printer can understand and translate into all its movements (g-code). The reason it's called \"slicing\" comes from the fact that a 3D print is created layer-by-layer, so what the slicer software actually does (among other things) is slicing the 3D model into thin layers. During slicing is when one can tamper with the way the final print will look. This is done through a very large amount of parameters that you can change to your liking. Usually it is better to keep the most parameters to the default settings, but there are some basic ones that you will usually want or have to edit.","title":"Slicing"},{"location":"3D-printing-101/#_1","text":"","title":""},{"location":"3D-printing-101/#printing-parameters","text":"","title":"Printing parameters"},{"location":"3D-printing-101/#this-is-a-long-detailed-read-only-if-you-care-about-quality-and-printing-duration","text":"Printing parameters are usually split into the below categories: * Printer settings * Fillament settings * Print settings","title":"(This is a long detailed read, only if you care about quality and printing duration)"},{"location":"3D-printing-101/#printer-settings","text":"Depending on the printer features, the printer settings must be set so the g-code created by the slicer is made for the printer you will use. This means that there is no way the printer will try to do something it can't or shouldn't do, like trying to move beyond its boundaries or hit its nozzle on an object. The most important settings are: * Printing bed size: Defining the bed dimensions (width, depth & height) so the software knows the maximum physical size of a model the printer can handle. * Number of extruders: Most printers have a single extruder, but there are some that have two. If you use 2 extruders, you must define their relative position by giving an offset value from their center. (Dual extruders are out of the scope of this guide) * Nozzle diameter: Usually by default the nozzle diameter is 0.4mm, but it is possible to use other sizes. Note that changing the nozzle diameter setting has to be done simultaneously with changing the physical printer nozzle.","title":"Printer settings"},{"location":"3D-printing-101/#fillament-settings","text":"These settings apply to the fillament material of the user. So, every time you try to change printing material you must change fillament settings (you can save and name settings to use them again). This may even be needed for the same material, but different brand. Typical settings to change are: * Color * Diameter: The diameter of the fillament before it enders the extruder. This typically is 1.75mm. * Extruder temperature: The temperature of the extruder must be set so that the fillament is properly melted in order to take the shape the printer gives it. The recommended value is written on the fillament package, but this is one of the most common settings to check/change when a print fails, because it is usually affected by environmental conditions. * Bed temperature: The bed temperature is set so the first layers of the print are still soft enough (but not melted) so that the print is stuck on the bed until the process finishes. This means the print will not get itself unstuck due to motion and ruin the process. The recommended value is written on the fillament package, but this is one of the most common settings to check/change when a print fails, because it is usually affected by environmental conditions. Both bed and extruder temperature can be set differently for the first and the rest layers.","title":"Fillament settings"},{"location":"3D-printing-101/#print-settings","text":"These settings can change mostly depending on the model geometry in order to properly support it. Typical settings to change are: Layer height: The layer height defines the thickness of each layer and accordingly the number of layers for the same model. This can affect the final look of the print, as thinner layers mean less ridges on the outside. But, the downside is the more layers, the longer the print.(the first layer can have different height, usually bigger, so it can stick more to the bed) Vertical and horizontal shells: The printed model has an outer surface and a filling pattern. The outer surface (or shell) can have multiple layers so it is smoother and more rigid. Infill: The infill is the way the printer completes the volume inside the outer surfaces. You can edit the density , pattern and angle of infill. Infill density defines what percentage of the volume is filled. High infill gives more rigid structures but substancially higher print times. Infill pattern controls the shape the infill has inside this volume. It can be a simple 2d pattern like squares or honeycombs same for each layer, or it can be a 3d pattern like waves or 3d honeycombs. This setting and the infill angle can affect the directional properties of a print. For instance to be able to handle more force in a certain direction. Print speed: This setting can be changed manually during print or set as a parameter during slicing. The print speed setting is modular. There can be a different setting for seperate layers, for support material, for skirts, brims, etc. So depending on your level of understanding print fails, you can change speed to correct specific problems. Support material: Creates supports for overhanging geometries. Changing support material settings can control the pattern of printing supports, how dense the layers are (meaning how easy it is to remove), etc. Skirt: Skirt is a perimeter around the print that doesn't have contact with it. It is used to make sure that the fillament is flowing correctly before beginning the print. Brim: Brim is a surface added around the first layer of the print in order to provide bigger stability for the next layers. Raft: It has the same functionality with the brim, but it extends to several layers and goes under the print. This means the first layer has only raft layers and the model layers begin after the raft has been made.","title":"Print settings"},{"location":"3D-printing-101/#common-materials","text":"PLA: The most common material. Cheaper, easier to print, biodegradable (debatable, though), hard and brittle. Indicative nozzle temp 190\u00b0C~205\u00b0C, indicative bed temp 55\u00b0C~65\u00b0C. ABS: Second most common material. Almost as cheap as PLA, a lot harder to print than PLA (sensitive to temperature changes; Can warp easily), acetone corrodes it, so it can be left in an acetone vat to be smoothed, hard and kind of flexible. Indicative nozzle temp 240\u00b0C~270\u00b0C, indicative bed temp 100\u00b0C~110\u00b0C. Thanks to @sspikeo.","title":"Common materials"},{"location":"Playground/","text":"Hello, is this a question?","title":"Playground"},{"location":"Areas/BJJ/Explosiveness/","text":"","title":"Explosiveness"},{"location":"Areas/BJJ/Flexibility/","text":"Active Isolated Stretching 30 Minute Jiu Jitsu (BJJ) Flexibility Routine (FOLLOW ALONG) - YouTube Neck rotations Thoracic rotations Sleeper stretch Thread the needle Tabletop lift Lying pike rotations Supine tailor rock 90:90 Tailor pose Horse squat to pancake Side pancake Upward dog Low lunge extension Camel pose Hero pose Child pose","title":"Flexibility"},{"location":"Areas/Julia/ModelingToolkit.jl/","text":"Description \u00b6 ModelingToolkit.jl is a modeling language. It can do both symbolic and numeric computation. It is highly performant and parallel. It is extendable because it brings ideas from symbolic CAS and causal/acausal equation-based modeling frameworks. The model can be input as a high-level description. Then, the model is analyzed and enhanced through symbolic preprocessing. It allows for automatic transformations, such as index reduction, to be applied before solving in order to easily handle equations that could not have been solved without symbolic intervention. It mixes symbolic computing packages like SymPy or Mathematica with equation-based modeling systems like the causal Simulink and the acausal Modelica . It can easily transform to and from different kinds of equations like DAEs into optimization problems and vice-versa. All the symbolic systems have a direct conversion to a numerical system which can then be handled through the SciML interfaces. Before producing code, it simplifies and parallelizes the model. Home \u00b7 ModelingToolkit.jl Features \u00b6 Causal and acausal modeling ( Simulink / Modelica ) Automated model transformation, simplification, and composition Automatic conversion of numerical models into symbolic models Composition of models through the components, a lazy connection system, and tools for expanding/flattening Pervasive parallelism in symbolic computations and generated functions Transformations like alias elimination and tearing of nonlinear systems for efficiently numerically handling large-scale systems of equations The ability to use the entire Symbolics.jl CAS as part of the modeling process. Import models from common formats like SBML , CellML , BioNetGen , and more. Extendability: the whole system is written in pure Julia , so adding new functions, simplification rules, and model transformations has no barrier. Feature List \u00b7 ModelingToolkit.jl Equation Types \u00b6 Ordinary differential equations Stochastic differential equations Partial differential equations Nonlinear systems Optimization problems Continuous-Time Markov Chains Chemical Reactions (via Catalyst.jl ) Nonlinear Optimal Control Standard Library \u00b6 See ModelingToolkitStandardLibrary.jl Model Import Formats \u00b6 CellML \u00b6 CellMLToolkit.jl can be used to import CellML models into ModelingToolkit. CellML is an XML-based open standard for mathematical models. It has a repository of more than a thousand models. While it's domain-agnostic, there's a focus on biomedical models. Currently, some of its active categories include: Cell Cycle Cell Migration Circadian Rhythms Gene Regulation Immunology Ion Transport Mechanical Constitutive Laws Metabolism Neurobiology PK/PD (pharmacokinetic/pharmacodynamic) Protein Modules Signal Transduction Synthetic Biology CellMLToolkit.jl imports a CellML model in XML and outputs a ModelingToolkit.jl IR (Intermediate Representation) to work with using the general SciML ecosystem. SBML \u00b6 SBMLToolkit.jl can be used to import SBML (Systems Biology Markup Language) models into ModelingToolkit and the general SciML ecosystem. It uses SBML.jl which in turn provides a Julia wrapper to libSBML . BioNetGen \u00b6 You can use SBMLToolkit \u2014 more specifically, the writeSBML() export function. BioNetGen is for biochemical reaction networks modeling. Thus, you can use ReactionNetworkImporters.jl . You can also use ReactionNetworkImporters.jl to import: networks represented by dense or sparse substrate and product stoichiometric matrices networks represented by dense or sparse complex stoichiometric and incidence matrices Notice that you can still use SBMLToolkit.jl, since you can export a model in SBML through BioNetGen (and COPASI , and Virtual Cell , etc.). Extension Libraries \u00b6 There's a lot of libraries that add features to the general equation-based modeling ecosystem that has ModelingToolkit as its core. Some are: Catalyst.jl : Symbolic representations of chemical reactions DataDrivenDiffEq.jl : Automatic identification of equations from data MomentClosure.jl : Automatic transformation of ReactionSystems into deterministic systems ReactionMechanismSimulator.jl : Simulating and analyzing large chemical reaction mechanisms NumCME.jl : High-performance simulation of chemical master equations (CME) FiniteStateProjection.jl : High-performance simulation of CMEs via finite state projections Compatible Numerical Solvers \u00b6 Again, all of the symbolic systems have a direct conversion to a numerical system which can then be handled through the SciML interfaces. For example, after building a model and performing symbolic manipulations, an ODESystem can be converted into an ODEProblem to then be solved by a numerical ODE solver. Below is a list of the solver libraries which are the numerical targets of the ModelingToolkit system: DifferentialEquations.jl : Multi-package interface of high performance numerical solvers for ODESystem , SDESystem , and JumpSystem NonlinearSolve.jl : High performance numerical solving of NonlinearSystem Optimization.jl : Multi-package interface for numerical solving OptimizationSystem NeuralPDE.jl : Physics-Informed Neural Network (PINN) training on PDESystem MethodOfLines.jl : Automated finite difference method (FDM) discretization of PDESystem","title":"ModelingToolkit.jl"},{"location":"Areas/Julia/ModelingToolkit.jl/#description","text":"ModelingToolkit.jl is a modeling language. It can do both symbolic and numeric computation. It is highly performant and parallel. It is extendable because it brings ideas from symbolic CAS and causal/acausal equation-based modeling frameworks. The model can be input as a high-level description. Then, the model is analyzed and enhanced through symbolic preprocessing. It allows for automatic transformations, such as index reduction, to be applied before solving in order to easily handle equations that could not have been solved without symbolic intervention. It mixes symbolic computing packages like SymPy or Mathematica with equation-based modeling systems like the causal Simulink and the acausal Modelica . It can easily transform to and from different kinds of equations like DAEs into optimization problems and vice-versa. All the symbolic systems have a direct conversion to a numerical system which can then be handled through the SciML interfaces. Before producing code, it simplifies and parallelizes the model. Home \u00b7 ModelingToolkit.jl","title":"Description"},{"location":"Areas/Julia/ModelingToolkit.jl/#features","text":"Causal and acausal modeling ( Simulink / Modelica ) Automated model transformation, simplification, and composition Automatic conversion of numerical models into symbolic models Composition of models through the components, a lazy connection system, and tools for expanding/flattening Pervasive parallelism in symbolic computations and generated functions Transformations like alias elimination and tearing of nonlinear systems for efficiently numerically handling large-scale systems of equations The ability to use the entire Symbolics.jl CAS as part of the modeling process. Import models from common formats like SBML , CellML , BioNetGen , and more. Extendability: the whole system is written in pure Julia , so adding new functions, simplification rules, and model transformations has no barrier. Feature List \u00b7 ModelingToolkit.jl","title":"Features"},{"location":"Areas/Julia/ModelingToolkit.jl/#equation-types","text":"Ordinary differential equations Stochastic differential equations Partial differential equations Nonlinear systems Optimization problems Continuous-Time Markov Chains Chemical Reactions (via Catalyst.jl ) Nonlinear Optimal Control","title":"Equation Types"},{"location":"Areas/Julia/ModelingToolkit.jl/#standard-library","text":"See ModelingToolkitStandardLibrary.jl","title":"Standard Library"},{"location":"Areas/Julia/ModelingToolkit.jl/#model-import-formats","text":"","title":"Model Import Formats"},{"location":"Areas/Julia/ModelingToolkit.jl/#cellml","text":"CellMLToolkit.jl can be used to import CellML models into ModelingToolkit. CellML is an XML-based open standard for mathematical models. It has a repository of more than a thousand models. While it's domain-agnostic, there's a focus on biomedical models. Currently, some of its active categories include: Cell Cycle Cell Migration Circadian Rhythms Gene Regulation Immunology Ion Transport Mechanical Constitutive Laws Metabolism Neurobiology PK/PD (pharmacokinetic/pharmacodynamic) Protein Modules Signal Transduction Synthetic Biology CellMLToolkit.jl imports a CellML model in XML and outputs a ModelingToolkit.jl IR (Intermediate Representation) to work with using the general SciML ecosystem.","title":"CellML"},{"location":"Areas/Julia/ModelingToolkit.jl/#sbml","text":"SBMLToolkit.jl can be used to import SBML (Systems Biology Markup Language) models into ModelingToolkit and the general SciML ecosystem. It uses SBML.jl which in turn provides a Julia wrapper to libSBML .","title":"SBML"},{"location":"Areas/Julia/ModelingToolkit.jl/#bionetgen","text":"You can use SBMLToolkit \u2014 more specifically, the writeSBML() export function. BioNetGen is for biochemical reaction networks modeling. Thus, you can use ReactionNetworkImporters.jl . You can also use ReactionNetworkImporters.jl to import: networks represented by dense or sparse substrate and product stoichiometric matrices networks represented by dense or sparse complex stoichiometric and incidence matrices Notice that you can still use SBMLToolkit.jl, since you can export a model in SBML through BioNetGen (and COPASI , and Virtual Cell , etc.).","title":"BioNetGen"},{"location":"Areas/Julia/ModelingToolkit.jl/#extension-libraries","text":"There's a lot of libraries that add features to the general equation-based modeling ecosystem that has ModelingToolkit as its core. Some are: Catalyst.jl : Symbolic representations of chemical reactions DataDrivenDiffEq.jl : Automatic identification of equations from data MomentClosure.jl : Automatic transformation of ReactionSystems into deterministic systems ReactionMechanismSimulator.jl : Simulating and analyzing large chemical reaction mechanisms NumCME.jl : High-performance simulation of chemical master equations (CME) FiniteStateProjection.jl : High-performance simulation of CMEs via finite state projections","title":"Extension Libraries"},{"location":"Areas/Julia/ModelingToolkit.jl/#compatible-numerical-solvers","text":"Again, all of the symbolic systems have a direct conversion to a numerical system which can then be handled through the SciML interfaces. For example, after building a model and performing symbolic manipulations, an ODESystem can be converted into an ODEProblem to then be solved by a numerical ODE solver. Below is a list of the solver libraries which are the numerical targets of the ModelingToolkit system: DifferentialEquations.jl : Multi-package interface of high performance numerical solvers for ODESystem , SDESystem , and JumpSystem NonlinearSolve.jl : High performance numerical solving of NonlinearSystem Optimization.jl : Multi-package interface for numerical solving OptimizationSystem NeuralPDE.jl : Physics-Informed Neural Network (PINN) training on PDESystem MethodOfLines.jl : Automated finite difference method (FDM) discretization of PDESystem","title":"Compatible Numerical Solvers"},{"location":"Areas/Julia/ModelingToolkitStandardLibrary.jl/","text":"","title":"ModelingToolkitStandardLibrary.jl"},{"location":"Areas/Julia/JuliaHub/Model%20Calibration%20and%20Parameter%20Estimation%20with%20JuliaSim%20Model%20Optimizer/","text":"TODO: for blog posts do acronyms as in here . Some time ago I attended a remote workshop titled \"Model Calibration and Parameter Estimation with JuliaSim Model Optimizer\" by the JuliaHub team, specifically Jacob Vaverka and Dr. Christopher Rackauckas . JuliaSim is a cloud-hosted platform for physical simulation. It combines a vast array of bleeding edge SciML techniques, acausal equation-based digital twin modeling and simulation and is powered by the Julia programming language. It is preview-only software in the time of writing this post (December 2022). JuliaSim produces surrogates of blackbox (and regular) dynamical systems using Continuous Time Echo State Networks (CTESNs). This technique allows, amongst other features, for implicit training in parameter space to stabilize the ill-conditioning present in stiff systems. You can leverage the surrogates to accelerate the process, there's a variety of techniques for quantifying uncertainty and noise (see the virtual populations below). You can use JuliaSim for parameter estimation and optimal control, which is what this post is about. There's the so-called Model Library , a collection of acausal (equation-based) components with pre-trained surrogates of models that are ready to use. You can thus discover and import/exchange various models, and combine yours with pre-built models and digital twins. Lastly, there's specialized numerical environments available for use upon demand. Everything can happen on the JuliaHub cloud-based IDE. If you want to build models, you can use the pre-made model libraries, e.g. CellMLPhysiome.jl and SBMLBioModels.jl . You can use the Catalyst.jl and ModelingToolkit.jl GUIs. If you want to generate models using existing data, you can use a Digital Twin generator. If you want to generate data using existing models, you can use a surrogatizer and more: ModelOptimizer: package methodology to perform model calibration, analysis, HPC environment, user-friendly manner. Robust and automated framework to scale large and complex models It can be difficult to run optimization techniques in distributed fashion, so you're oftentimes constrained to serial implementations which is costly from many aspects. Some model parameters may be unidentifiable from the data. To address the first challenge \u2014 how do we avoid local optima? Leverage specialized methods from Model Optimizer. There are many calibration methods available, some of which are below. Which one you choose is going to depend at the specific problem at hand. To address the second challenge \u2014 how do you do effective parallelism on a particular strategy that you're deploying, how do you leverage large-scale cloud compute systems to solve these problems? The proper strategy selection plays a big role here. You can enable parallelism with certain strategies. Multiple shooting is one example where if it's an effective strategy it helps you break out the serial of execution. Some of the many available techniques are more amenable to distributed compute. To address the third challenge \u2014 how do you quantify the uncertainty in the fit? with introducing the concept of virtual populations. Virtual populations are sets of parameters which sufficiently fit all trials, all observations. A trial is a different variation of the model which can be ran, it describes an experiment . It's a data observation. We can have collections of trials, multiple trials. With the collection we can define multi-simulation optimization problems. This is the problem. Before we really get into seeing this in action, we need to discuss different modeling paradigms. How do we create these simulations and have something that we can apply these model optimizer techniques to? Model paradigms \u00b6 Causal Modeling \u00b6 So, in causal modeling, we describe the causal mechanisms of a system. The way that this works is we provide clear rules for the interactions between functional blocks. We can make an analogy to imperative programming, we're worried about the flow of computation. This leads us nicely into Acausal modeling \u00b6 Describe the behavior and the properties of the model components and then models are built up out of the composition of the components. The overall dynamics of the model fall out of the cumulative behavior of the composition. Here, let's employ the declarative programming analogy, we only worry about the connections and the relationships between these functional blocks, we don't want to frame the problem particularly in terms of the flow of computation that has to happen, we want to think instead about individual components and the relationships between one another. Acausal modeling can be expressive. We can think like scientists and engineers and we're not limited in framing the problem only in terms of how to compute the results. We can be concise, so we can build large-scale models by connecting well-tested components. We can be reusable in that we can bring these well-tested components and entire component models with us to build new systems. ModelingToolkit.jl is a Julia acausal modeling framework (TODO: ModelingToolkit.jl ) and it will allow us to be expressive and concise when we write our DE models. It will also enable us to reuse these models so we can automatically rearrange equations for better stability. We're gonna get some extra perks here. We're gonna get optimal code by default, without having to worry about the most optimal way to compute these things we just worry about the mechanics and then we get the optimal code for free. The code will also be parallelizable by default. We're also going to use some components from the ModelingToolkit Standard Library (TODO: ModelingToolkit.jl ). So, speaking of reusability, we're gonna leverage some pre-built components from the standard library and this is going to help us dive directly into the engineering and not focus as much on the math and the programming of building all these things up from scratch. So this is exactly the intended purpose of this library. A Chua circuit is a simple electrical circuit (TODO: check GitHub) that exhibits chaotic behavior. It's easy to construct and it's a classic real world example of what a chaotic system can look like. In order to get this chaotic behavior, we need to satisfy a couple requirements. We need at least one nonlinear element if you look at the diagram, and that's what $N_R$ stands for, it's going to be our nonlinear resistor. We need at least one locally active resistor, also $N_R$ in the diagram, and then we need at least three energy storage elements, so that's where the capacitors $C_1$ and $C_2$ and the inductor $L$ come into play. This is the diagram we're going to be building. First, we load some packages. JuliaSimModelOptimizer is the first package. We're going to be pulling from the Electrical module from the standard library, and we have some various packages here. First we need to define the components that are not readily available in the standard library (TODO: SynBio link!). So first we need a parameter for time, and then we can build the nonlinear resistor. So this is the code representation of exactly what was shown in the diagram. You can see we're using some components from the electrical module, and then it's pretty straightforward to describe the various equations that we want to govern this nonlinear resistor's behavior. There's a couple nested ifelse statements which employ different equations based on different conditions of the nonlinear resistor. What we return from this function here is an ODESystem which is going to help us create the component. NonlinearResistor was the only component we needed to build ourselves. So, after defining model parameters not readily available in the model library, we can now create the model components (using the standard library components and the nonlinear resistor we built). Inductor , Resistor , Conductor and Ground are all from the electrical module of the standard library. We can create the model components with the same labels we saw on the diagram. Once we have each of these elements, we can start defining the relationships between the components. That's what you see in the connect statements. When talking about not necessarily having to worry about writing the optimal code, but instead just describing the relationships between each of the components, this is what you see here. We can connect the inductor to the ground, the nonlinear resistor to the ground, the capacitor to the ground, we can do all of this with connect statements and at the end we feed all of these connections, so all of these equations along with the ODESystem that we've created, put it in an element called model, and that's going to give us everything needed. But! We didn't pay close attention at all in making sure this runs in an optimal way, so that's where structural_simplify is a very handy tool. We can pass our model into this structural_simplify call. It is going to simplify the algebraic equations of the system, so wherever possible, it's going to not make redundant calls, it's going to find the fastest method to solve this model for us. TODO: look into this. So if we just call it we can get the optimal version of the model for free. After we get that, we can create our ODEProblem run it over a particular timespan, and give it some instructions to save at certain times (mostly for plotting later), and then we can solve the problem. Now we can move on with defining the inverse problem (up to this problem model optimizer wasn't needed. Add things on that and on what the inverse problem is). The first step here is typically we would load the experimental data. Typically you might see something here like using the CSV package read in a file where i had all the experiment data saved and then i want to tie this data back to the model. In this example we're going to use the solution that we got directly above from calling solve on our problem. We have everything needed to define a trial. A trial is the model plus our data. We just pass in the data, the system and tell it what time to run over. Now that we have a trial, we can create the inverse problem. We're specifying the parameters that we want to optimize and we also specify the search space we wanna look for. We're passing a collection of trials, granted it's a single trial in this collection, but it's still a collection. We pass the model. Then, we pass the search space. You can see here the model parameter, the lower and the upper bound for each. What it's gonna do, is it's going to find the model parameters in the system, it's gonna find the parameters which cause the model to be a sufficiently good fit to all data, all data here being the collection of trials that we've passed. Since that's done, we move on to the next block here which is the virtual population. It's the plausible population of optimal points. So, creating this virtual population is as easy as feeding in the inverse problem, passing an optimization algorithm, in this case we're using the stochastic global optimization algorithm, we're gonna do a max iteration of 1000, 5000 is a much more realistic number to use. This DataFrame is produced: Note that when we defined our capacitors, C1 and C2 we passed in values for C . You can see in the results of the virtual population (dataframe) there is parameter c, for c1 we passed in 10 you can see many of these values are right around 10. That's a good sign, it means we're getting close to that value, same thing for C2 which is at 100. This is an eyeball test. We can then run these visualizations for a more clear way to see these results. We're going to create a single image which is a layout of three images and for each one we're gonna use a smooth density plot to show the virtual population. Then since we know the true value we can plot it too to use for comparison. We can see that even with a low number of iterations we're still capturing the true values pretty well in the virtual populations. To reiterate, we first created the model with ModelingToolkit, the acausal modeling framework, we had to create only one component from scratch, we were able to leverage the oneport component from the electrical module. We connected all of the model components, we got some optimized code, and then we used that to generate some synthetic data and create this inverse problem. Directly from there, we can create this virtual population. This is the whirlwind overview of what a Model Optimizer workflow may look like. Let's take another look at an example of a building model. This building model is an 81-room building, it's 9 by 3 by 3, cooled by circulating water, in the graphs is data from real meteorological data. The image on the left, the building, correlates to the vertical line moving across the graphs on the right. On the top we get the room temperature for each room, on the bottom the ambient temperature. There are 7 peaks and valleys on the bottom, which correlates with 7 sunrises and falls during a week. We can see the time of day and how it affects both ambient temperature and room temperature. How might we construct a model like this with an acausal modeling framework like modelingtoolkit? After we load some packages, like modelingtoolkit, differentialequations, some things to handle experimental data like DataFrames and CSV, a data interpolation package, we're also going to pull a package of components that we've developed to break down this problem into smaller portions, very similar to a standard library. So, after we've done that, and we load our meteorological data and slice the data, get everything ready, we can start setting our structural parameters for the model and the building. We set some initial temperatures for the building, the temperatures are measured in kelvin in the model. After we've also set our initial conditions, we can do some component setup. Very similar to how we would just pull in the capacitor, the inductor, the ground source, we can pull in these interpolated sources, constant sources, step sources, we have a function to create a HEX fan there. We can create each one of these components and we can create many of them in one go, which is this array syntax. Then, just like before, instead of writing this massive model by hand, which comes out to be over 17000 equations, we can focus purely on the relationships between these components. So this model is built up by the composition of each one of these components seen before. The overall dynamics of this building model fall out of the cumulative behavior of that composition. Of all the individual components, of all their dynamics, and of all the connections that we've declared here you can get the dynamics of the full model. We only worry about the relationships, the connections between these things. This happens to happen in a concise, fast way because we're able to do this in a for loop so we're gonna go over each room, set up these equations, these connections for every room in the building without having to code this by hand. The output is an ODESystem. Loading the model directly from data now. Note there's 2619 equations, a massive reduction from the 17000 equations we started with. The model we get out of the initialize_model call has already went through all the various reduction algorithms to get this structurally and algebraically simplified. We now got a nice lean model to work with, we now want to define some model parameters, this is where the unit conversion package comes into nicely, seconds to hours, kelvin to Celsius, leaves little room for developer error. We're going to do the same 3 steps. Load the experimental data, define our trials, then define the inverse problem. Then, we can calibrate the model. calibrate comes from model optimizer. Then, we just convert the results from Kelvin to Celsius and we have the parameter estimation for this model. All these methods shown here are designed for large-scale simulations to fit to data. It's a form of curve fitting that is made for robustness to nonlinear behavior. In the questions \"how do you do this chaotic system\", they are well-founded. Because for example, chaotic system, you have O(1) error in your simulation, if you simulate it for multiple lyapunov times. The techniques here are robust to these behaviors. They make use of multiple shooting and they make use of things like collocation to be able to be fitting in the derivative space in a way that does not have this compounding of errors. If you naively slap an ODE solver into BFGS and then just directly fit and then hope and pray, a chaotic system is an example of where this approach would fail. This building example shows the robustness of the methods.","title":"Model Calibration and Parameter Estimation with JuliaSim Model Optimizer"},{"location":"Areas/Julia/JuliaHub/Model%20Calibration%20and%20Parameter%20Estimation%20with%20JuliaSim%20Model%20Optimizer/#model-paradigms","text":"","title":"Model paradigms"},{"location":"Areas/Julia/JuliaHub/Model%20Calibration%20and%20Parameter%20Estimation%20with%20JuliaSim%20Model%20Optimizer/#causal-modeling","text":"So, in causal modeling, we describe the causal mechanisms of a system. The way that this works is we provide clear rules for the interactions between functional blocks. We can make an analogy to imperative programming, we're worried about the flow of computation. This leads us nicely into","title":"Causal Modeling"},{"location":"Areas/Julia/JuliaHub/Model%20Calibration%20and%20Parameter%20Estimation%20with%20JuliaSim%20Model%20Optimizer/#acausal-modeling","text":"Describe the behavior and the properties of the model components and then models are built up out of the composition of the components. The overall dynamics of the model fall out of the cumulative behavior of the composition. Here, let's employ the declarative programming analogy, we only worry about the connections and the relationships between these functional blocks, we don't want to frame the problem particularly in terms of the flow of computation that has to happen, we want to think instead about individual components and the relationships between one another. Acausal modeling can be expressive. We can think like scientists and engineers and we're not limited in framing the problem only in terms of how to compute the results. We can be concise, so we can build large-scale models by connecting well-tested components. We can be reusable in that we can bring these well-tested components and entire component models with us to build new systems. ModelingToolkit.jl is a Julia acausal modeling framework (TODO: ModelingToolkit.jl ) and it will allow us to be expressive and concise when we write our DE models. It will also enable us to reuse these models so we can automatically rearrange equations for better stability. We're gonna get some extra perks here. We're gonna get optimal code by default, without having to worry about the most optimal way to compute these things we just worry about the mechanics and then we get the optimal code for free. The code will also be parallelizable by default. We're also going to use some components from the ModelingToolkit Standard Library (TODO: ModelingToolkit.jl ). So, speaking of reusability, we're gonna leverage some pre-built components from the standard library and this is going to help us dive directly into the engineering and not focus as much on the math and the programming of building all these things up from scratch. So this is exactly the intended purpose of this library. A Chua circuit is a simple electrical circuit (TODO: check GitHub) that exhibits chaotic behavior. It's easy to construct and it's a classic real world example of what a chaotic system can look like. In order to get this chaotic behavior, we need to satisfy a couple requirements. We need at least one nonlinear element if you look at the diagram, and that's what $N_R$ stands for, it's going to be our nonlinear resistor. We need at least one locally active resistor, also $N_R$ in the diagram, and then we need at least three energy storage elements, so that's where the capacitors $C_1$ and $C_2$ and the inductor $L$ come into play. This is the diagram we're going to be building. First, we load some packages. JuliaSimModelOptimizer is the first package. We're going to be pulling from the Electrical module from the standard library, and we have some various packages here. First we need to define the components that are not readily available in the standard library (TODO: SynBio link!). So first we need a parameter for time, and then we can build the nonlinear resistor. So this is the code representation of exactly what was shown in the diagram. You can see we're using some components from the electrical module, and then it's pretty straightforward to describe the various equations that we want to govern this nonlinear resistor's behavior. There's a couple nested ifelse statements which employ different equations based on different conditions of the nonlinear resistor. What we return from this function here is an ODESystem which is going to help us create the component. NonlinearResistor was the only component we needed to build ourselves. So, after defining model parameters not readily available in the model library, we can now create the model components (using the standard library components and the nonlinear resistor we built). Inductor , Resistor , Conductor and Ground are all from the electrical module of the standard library. We can create the model components with the same labels we saw on the diagram. Once we have each of these elements, we can start defining the relationships between the components. That's what you see in the connect statements. When talking about not necessarily having to worry about writing the optimal code, but instead just describing the relationships between each of the components, this is what you see here. We can connect the inductor to the ground, the nonlinear resistor to the ground, the capacitor to the ground, we can do all of this with connect statements and at the end we feed all of these connections, so all of these equations along with the ODESystem that we've created, put it in an element called model, and that's going to give us everything needed. But! We didn't pay close attention at all in making sure this runs in an optimal way, so that's where structural_simplify is a very handy tool. We can pass our model into this structural_simplify call. It is going to simplify the algebraic equations of the system, so wherever possible, it's going to not make redundant calls, it's going to find the fastest method to solve this model for us. TODO: look into this. So if we just call it we can get the optimal version of the model for free. After we get that, we can create our ODEProblem run it over a particular timespan, and give it some instructions to save at certain times (mostly for plotting later), and then we can solve the problem. Now we can move on with defining the inverse problem (up to this problem model optimizer wasn't needed. Add things on that and on what the inverse problem is). The first step here is typically we would load the experimental data. Typically you might see something here like using the CSV package read in a file where i had all the experiment data saved and then i want to tie this data back to the model. In this example we're going to use the solution that we got directly above from calling solve on our problem. We have everything needed to define a trial. A trial is the model plus our data. We just pass in the data, the system and tell it what time to run over. Now that we have a trial, we can create the inverse problem. We're specifying the parameters that we want to optimize and we also specify the search space we wanna look for. We're passing a collection of trials, granted it's a single trial in this collection, but it's still a collection. We pass the model. Then, we pass the search space. You can see here the model parameter, the lower and the upper bound for each. What it's gonna do, is it's going to find the model parameters in the system, it's gonna find the parameters which cause the model to be a sufficiently good fit to all data, all data here being the collection of trials that we've passed. Since that's done, we move on to the next block here which is the virtual population. It's the plausible population of optimal points. So, creating this virtual population is as easy as feeding in the inverse problem, passing an optimization algorithm, in this case we're using the stochastic global optimization algorithm, we're gonna do a max iteration of 1000, 5000 is a much more realistic number to use. This DataFrame is produced: Note that when we defined our capacitors, C1 and C2 we passed in values for C . You can see in the results of the virtual population (dataframe) there is parameter c, for c1 we passed in 10 you can see many of these values are right around 10. That's a good sign, it means we're getting close to that value, same thing for C2 which is at 100. This is an eyeball test. We can then run these visualizations for a more clear way to see these results. We're going to create a single image which is a layout of three images and for each one we're gonna use a smooth density plot to show the virtual population. Then since we know the true value we can plot it too to use for comparison. We can see that even with a low number of iterations we're still capturing the true values pretty well in the virtual populations. To reiterate, we first created the model with ModelingToolkit, the acausal modeling framework, we had to create only one component from scratch, we were able to leverage the oneport component from the electrical module. We connected all of the model components, we got some optimized code, and then we used that to generate some synthetic data and create this inverse problem. Directly from there, we can create this virtual population. This is the whirlwind overview of what a Model Optimizer workflow may look like. Let's take another look at an example of a building model. This building model is an 81-room building, it's 9 by 3 by 3, cooled by circulating water, in the graphs is data from real meteorological data. The image on the left, the building, correlates to the vertical line moving across the graphs on the right. On the top we get the room temperature for each room, on the bottom the ambient temperature. There are 7 peaks and valleys on the bottom, which correlates with 7 sunrises and falls during a week. We can see the time of day and how it affects both ambient temperature and room temperature. How might we construct a model like this with an acausal modeling framework like modelingtoolkit? After we load some packages, like modelingtoolkit, differentialequations, some things to handle experimental data like DataFrames and CSV, a data interpolation package, we're also going to pull a package of components that we've developed to break down this problem into smaller portions, very similar to a standard library. So, after we've done that, and we load our meteorological data and slice the data, get everything ready, we can start setting our structural parameters for the model and the building. We set some initial temperatures for the building, the temperatures are measured in kelvin in the model. After we've also set our initial conditions, we can do some component setup. Very similar to how we would just pull in the capacitor, the inductor, the ground source, we can pull in these interpolated sources, constant sources, step sources, we have a function to create a HEX fan there. We can create each one of these components and we can create many of them in one go, which is this array syntax. Then, just like before, instead of writing this massive model by hand, which comes out to be over 17000 equations, we can focus purely on the relationships between these components. So this model is built up by the composition of each one of these components seen before. The overall dynamics of this building model fall out of the cumulative behavior of that composition. Of all the individual components, of all their dynamics, and of all the connections that we've declared here you can get the dynamics of the full model. We only worry about the relationships, the connections between these things. This happens to happen in a concise, fast way because we're able to do this in a for loop so we're gonna go over each room, set up these equations, these connections for every room in the building without having to code this by hand. The output is an ODESystem. Loading the model directly from data now. Note there's 2619 equations, a massive reduction from the 17000 equations we started with. The model we get out of the initialize_model call has already went through all the various reduction algorithms to get this structurally and algebraically simplified. We now got a nice lean model to work with, we now want to define some model parameters, this is where the unit conversion package comes into nicely, seconds to hours, kelvin to Celsius, leaves little room for developer error. We're going to do the same 3 steps. Load the experimental data, define our trials, then define the inverse problem. Then, we can calibrate the model. calibrate comes from model optimizer. Then, we just convert the results from Kelvin to Celsius and we have the parameter estimation for this model. All these methods shown here are designed for large-scale simulations to fit to data. It's a form of curve fitting that is made for robustness to nonlinear behavior. In the questions \"how do you do this chaotic system\", they are well-founded. Because for example, chaotic system, you have O(1) error in your simulation, if you simulate it for multiple lyapunov times. The techniques here are robust to these behaviors. They make use of multiple shooting and they make use of things like collocation to be able to be fitting in the derivative space in a way that does not have this compounding of errors. If you naively slap an ODE solver into BFGS and then just directly fit and then hope and pray, a chaotic system is an example of where this approach would fail. This building example shows the robustness of the methods.","title":"Acausal modeling"},{"location":"Areas/Julia/JuliaHub/Using%20Large%20Datasets%20in%20JuliaHub/","text":"Using Large Datasets in JuliaHub - YouTube","title":"Using Large Datasets in JuliaHub"},{"location":"Areas/Math/Markov%20Process/","text":"","title":"Markov Process"},{"location":"Areas/Photography/Film/","text":"","title":"Film"},{"location":"Areas/Photography/Mamiya%20ZE-2%20Quartz/","text":"Introduced in December 1980.","title":"Mamiya ZE 2 Quartz"},{"location":"Areas/School/Numerical%20Analysis/Intro/","text":"Criteria to select a numerical method: General: Complexity (computational, memory, implementation) Rate of convergence Accuracy Numerical stability","title":"Intro"},{"location":"Areas/iGEM/Task/","text":"Describe the problem the team decided to solve and the solution the team suggested (max 2000 characters). \u00b6 The UCopenhagen iGEM team designed to tackle the accumulation of what's termed ghost nets \u2014 fishing nets that end up lost drifting in the waters. Ghost nets are lethal to a variety of marine animals, including organisms close to the sediments. Nets are assembled from synthetic fibers which have a lifespan of several hundred years. Additionally, they break up into little bits \u2014 microplastics, which can be ingested by both wildlife and humans and are detrimental to health. Microplastics also intensify the greenhouse effect as they release gases during breakage and render microorganisms less effective in capturing CO2. All of the above constitute a pressing issue: to better contextualize the scale of this phenomenon, the ghost nets in the North Pacific Ocean total an area that's three times the area of France. To combat this problem, the iGEM team synthesized a biodegradable fiber based on spider-silk protein as an alternative to commercial plastic nets. Such proteins can refold into conformations dominated by beta sheets, making them very robust. The fiber core was then coated by protein found in clams which acts as an intermediate water-proof layer that is also resitant to bacterial degradation. A peptide-protein pair that forms an isopeptide bond and can form a fusion protein was employed to link the layers together. Furthermore, this lock-and-key mechanism can serve as a platform allowing for attaching additional proteins to the fiber, providing an opportunity to further enhance and tune its proteins. For example, the team explored adding a bioluminescence mechanism to repel endangered animals in time. Their experimental approaches were guided partly by implementing a model to simulate the conformational changes that occur when the soluble silk protein is spun into becoming fiber. Give one example on how this team reached out to the general public and one example on how they got feedback from the scientific community. \u00b6 https://2022.igem.wiki/ucopenhagen/communication : Workshops in different Eco-Schools with the purpose of communicating and working on the Sustainable Development Goals https://2022.igem.wiki/ucopenhagen/human-practices : Scientific integration , Problem 4 To outreach their efforts and findings in the general public, the team employed a multifaceted strategy. A key part of this approach was establishing communication with Eco-Schools \u2014 schools that are part of the largest global sustainable program targeted at young students. Through this collaboration, the team managed to raise awareness on the ghost nets matter, as well as showcasing how the field of Synthetic Biology can be applied to engineer effective solutions to achieve the long-sought after SDGs, or Sustainable Development Goals. The UCopenhagen group used minispidroin proteins to form the fiber core. Trying to spin the protein solution, the protocol they had developed was found lacking, since the protein was eventually observed to cluster into clumps instead of a proper fibre-shaped string. The students approached the inventor of minispidroins and another professor who is considered to be an expert in biopolymer spinning. They then adjusted both the protocol procedures (gently mixing the solution sporadically) and the equipment used (a long extrusion capillary to align the various fibres).","title":"Task"},{"location":"Areas/iGEM/Task/#describe-the-problem-the-team-decided-to-solve-and-the-solution-the-team-suggested-max-2000-characters","text":"The UCopenhagen iGEM team designed to tackle the accumulation of what's termed ghost nets \u2014 fishing nets that end up lost drifting in the waters. Ghost nets are lethal to a variety of marine animals, including organisms close to the sediments. Nets are assembled from synthetic fibers which have a lifespan of several hundred years. Additionally, they break up into little bits \u2014 microplastics, which can be ingested by both wildlife and humans and are detrimental to health. Microplastics also intensify the greenhouse effect as they release gases during breakage and render microorganisms less effective in capturing CO2. All of the above constitute a pressing issue: to better contextualize the scale of this phenomenon, the ghost nets in the North Pacific Ocean total an area that's three times the area of France. To combat this problem, the iGEM team synthesized a biodegradable fiber based on spider-silk protein as an alternative to commercial plastic nets. Such proteins can refold into conformations dominated by beta sheets, making them very robust. The fiber core was then coated by protein found in clams which acts as an intermediate water-proof layer that is also resitant to bacterial degradation. A peptide-protein pair that forms an isopeptide bond and can form a fusion protein was employed to link the layers together. Furthermore, this lock-and-key mechanism can serve as a platform allowing for attaching additional proteins to the fiber, providing an opportunity to further enhance and tune its proteins. For example, the team explored adding a bioluminescence mechanism to repel endangered animals in time. Their experimental approaches were guided partly by implementing a model to simulate the conformational changes that occur when the soluble silk protein is spun into becoming fiber.","title":"Describe the problem the team decided to solve and the solution the team suggested (max 2000 characters)."},{"location":"Areas/iGEM/Task/#give-one-example-on-how-this-team-reached-out-to-the-general-public-and-one-example-on-how-they-got-feedback-from-the-scientific-community","text":"https://2022.igem.wiki/ucopenhagen/communication : Workshops in different Eco-Schools with the purpose of communicating and working on the Sustainable Development Goals https://2022.igem.wiki/ucopenhagen/human-practices : Scientific integration , Problem 4 To outreach their efforts and findings in the general public, the team employed a multifaceted strategy. A key part of this approach was establishing communication with Eco-Schools \u2014 schools that are part of the largest global sustainable program targeted at young students. Through this collaboration, the team managed to raise awareness on the ghost nets matter, as well as showcasing how the field of Synthetic Biology can be applied to engineer effective solutions to achieve the long-sought after SDGs, or Sustainable Development Goals. The UCopenhagen group used minispidroin proteins to form the fiber core. Trying to spin the protein solution, the protocol they had developed was found lacking, since the protein was eventually observed to cluster into clumps instead of a proper fibre-shaped string. The students approached the inventor of minispidroins and another professor who is considered to be an expert in biopolymer spinning. They then adjusted both the protocol procedures (gently mixing the solution sporadically) and the equipment used (a long extrusion capillary to align the various fibres).","title":"Give one example on how this team reached out to the general public and one example on how they got feedback from the scientific community."},{"location":"Areas/iGEM/iGEM%20UCopenhagen%202022/","text":"","title":"iGEM UCopenhagen 2022"},{"location":"Projects/ESA%20Design%20Booster/","text":"ESA Design Booster Introduction \u00b6 Greetings! I'm Orestis, I come from Greece and I'm the co-science lead in the AcubeSAT project. AcubeSAT is a project of a student team called SpaceDot and is part of the Fly Your Satellite 3! program. I've been involved in this effort since almost the very beginnings, when everything was still at a nascent stage. I'm currently getting to wrap up my studies, and I've joined the team not long after I enrolled in the university, I've worked in different aspects of the project, technical and not, and now I'm responsible for the Science-y stuff. The AcubeSAT nanosatellite undertaking began late 2018 - 2019. We've designed and are developing a 3U CubeSat which holds a 2U biology payload. Our mission is two-fold: we want to establish our idea for a modular platform to perform space biology experiments in CubeSats/small satellites as a working prototype. Also, we aim to probe the way conditions in LEO (mainly microgravity and cosmic radiation) affect yeast cells at the gene expression level. To achieve that, the 2U payload is a pressurized vessel which hosts a container with the various compartments to run our experiments. We'll culture cells in-orbit and then see how their gene expression is altered through acquiring images via our DIY microscope-like imaging system. We aim to run the same experiment in three distinct timepoints across the duration of our mission, and to do this we've been using a small platform called a LoC to hold the cells and interface them with the various fluidics in a miniaturized version of a biology lab; but more on that at the end. In brief, we've submitted our proposal in October 2019, took part in the selection workshop hosted here in December 2019, got accepted in February 2020, submitted the TS-VCD (requirements) in April, submitted the first version of CDR in October and had it approved in September 2021, marking the end of the design phase and the beginning of the construction/testing phase. Right now we are getting ready to run a series of testing campaigns at the ESA facilities, then pass the MRR and be in-orbit by late 2023/early 2024. Context \u00b6 Before getting on with the actual presentation, I feel the need to underline the overall surrounding context regarding the team I've been and am a part of and the general environment in which we've been undertaking the FYS! journey. Maybe some of these will align with the environment and the challenges you are to face, maybe not. 1) There's no significant activity in the Aerospace sector in Greece, with the exception of some little companies and organizations that are trying to kickstart things, but we're still at a very early stage 2) On a similar manner, there's not much related expertise or a degree (e.g. in Aerospace engineering) to be offered in Greek universities. These mean it's difficult to get financial or technical support for the project 3) Student's don't gain ECTS credits etc. for being involved in the project to count towards completing a degree. People usually don't get to work at something space-related afterwards, at least in Greece, most enter the team stay for 1 - 1.5 year and then leave Presentation scope \u00b6 The presentation is focused on discussing some of the programmatic aspects of the challenges you might come to face as FYS! progresses, instead of getting into the nitty gritty things on the technical side. Because I wanted to cover a lot of different topics in a short amount of time, it will be a little free flowing. Presentation Structure \u00b6 The presentation is split into topics that fall mainly into two distinct categories, the first being project management, while the second follows a more people-centric narrative. There will be advice with an emphasis on the earlier stages of the project, and some good and... not that good decisions to make or avoid, respectively. I'll try cast a more personal light and share some hurdles we've faced and had to overcome, or that we are still facing to this day. Following that tangent, I'll briefly go over some particular examples that greatly affected our team. Then, I'll mention some additional tips, and will close the presentation with an addendum on trying to realize a mission carrying a biological payload, since I've been informed there are some teams interested in working towards that. Because this is somewhat of a niche, please feel free to come have a chat with me after the Q&A session is over, so that we can talk in more detail. I'll share some handles and ways you can reach out to me, I and the whole team are very open and eager to discuss and chime in in whichever way possible. Project management \u00b6 there's always exceptions to rules. Understand the rules, then break them - don't take what I say as a hard absolute to be followed. It's empirical data gathered through observation. Think about what I say and try to understand why I say it, throughout the talk Things are tough you have to develop skillset from zero - not just technical, but mostly the organizational almost everything is behind closed doors, you don't even know what's out there, you have to learn but how? you have to find support (e.g. from university) you have to ensure infrastructure (e.g. machines) - might be something very generic to get prototypes, might be something very specific related to your payload. Getting access to required infrastructure might be almost trivial in some cases, near impossible in others. You'll face this sooner or later, start thinking about it as early as possible invest in open-source and the community. There's people you can learn from, if not teach - talk a little bit about open-source and how it has helped us and others. In any case, do some research for what's out there. FYS! is about CubeSats. CubeSats are very affordable and easy to build and verify, which means in turn that they are very approachable and that a lot of people who are somehow involved with a similar mission do not belong in the space industry, meaning there's a wealth of information that isn't as strictly regulated you got help from ESA, experts (esp. the experts), reach out - a lot of experience that you can't find elsewhere. Don't be afraid to ask because it might seem like you haven't done your research; it's an educational program and they want you to succeed. Remember that they've went through the whole journey multiple times. This is very crucial to take advantage of as early as possible. They won't just help you solve problems, they might give you fundamentally new ideas and paths to explore other teams participate in FYS!; be in touch - in other FYS! versions and other related projects, we've helped and received helped bridge: you can get help, and you will need help, because: Scale up there's no leadership voice outside of the team, the organization has to be DIY - biggest factor towards your success, no one will help you, and you won't have expertise. It's one thing to learn how to blink LEDs, and another to learn how to run a team of 40 people to meet strict deadlines in a very demanding project COVID hit, and it hit hard. Why? - a good segue, we had to rethink our organizational structure due to limited physical access. A hybrid scheme works best you'll eventually have to scale up, time and time again. How? - your organization must not only stand the test of \"now\", but also be future proof find good infrastructure, platforms, esp. for organization - can't stress this enough. Communication, tracking and finding information, project planning and management. Don't be pushing things back because you have a deadline to meet. Don't be afraid to invest early, plant the seeds; they will bloom later. Information transfer try maintain information transfer at all costs - our biggest problem documentation is always needed after completion (Akin's law), it almost never gets written post. If you come to me after I announce that I'll leave and say hey, can you write documentation and/or train new members? It will never happen. It will be sub-optimal at best bridge: TODO: organization Organization: the small keep minutes, have agendas, don't recycle topics, end each topic with an action, make the most out of a meeting cross-team meetings are very important hold concurrent sessions, frequent meetings, be transparent always bridge: on a more general note, ... ... and the big you need SYE, aka systems overview: for management, always see the forest, not the tree - how do all the pieces interact with all the other pieces? always be wary of cross-dependencies - some work that needs to be done needs other work to be done first; or I'm needed at A but I'm also needed at B constantly track things, move deadlines appropriately, be on top of things - you'll come to find out that you'll be setting a deadline and it will be way overdue and then you'll have to set a deadline anew and do that because it helps you know where you are and where you need to at all times be agile, especially in the beginning. Find what's good for you. Understand why it's good for you - purposely talking in a loose frame and giving general comments, self-reflection is the most important thing bridge: exploring a path that you might later on decide that you won't follow, playing with the design, the structure, being bold. You have time, you can afford to make mistakes Calm down you'll screw up, constantly. Relax and learn from it - it sounds very cliche, but it holds so so true. We've had so many situations where things almost derailed and came close to crumbling down because people were losing their minds. Relax, be calm, assess the situation, do your best, learn from any mistakes and move on. In the grand scheme of things, it probably doesn't matter that much diffuse situations, avoid crowd mentality - we've had \"critical\" situations dozens of times, we're still here remember, everything's gonna be alright if it isn't, not to worry. It's an educational program - I'm not trying to set the bar low here, things are very challenging and you should strive for excellence, but you have to understand the nature of the program and that the ESA people are here to help and that they know you will commit a lot of mistakes. The thought that they expect you to make mistakes is very liberating, because you will make mistakes and you might think that the consequences will be way bigger than they actually will be. I've noticed that they even get anxious if time passes and you don't seem to be having any problems. Plus, having a problem is great because you know that something is wrong, you probably know what it is, and you will eventually figure out a way to fix things. That's way better than sitting awkwardly wondering whether there's a problem looming around the corner waiting to bite you when you're not looking try to build a momentum, expect a momentum to be built, then ride it, but guide it too; don't let it carry you - I keep saying that things are difficult; you'll find it difficult at first with a lot of things, for example with the organizational structure. It will require a lot of effort in the beginning, it's like a slope where, if you're doing it right, little by little it will get easier with time. Our case happened after we delivered the first CDR version successfully, things somewhat got automated, everyone knew what to work on, communication was frictionless, new members were seamlessly integrated, but you have to always keep things in check and revisit things. We did this mistake and cracks started to appear; it costs us a lot in the end. The bus factor be afraid of the bus factor . The bus factor is when ... and you will come across it and some times there won't be a lot of things you'll be able to do to mitigate it. But also always remember: NO ONE is irreplaceable - (\u03cc\u03c3\u03b5\u03c2 \u03c6\u03bf\u03c1\u03ad\u03c2 \u03b5\u03af\u03c0\u03b1\u03bc\u03b5 \u03bd\u03b1 \u03c0\u03ac\u03bc\u03b5 \u03bc\u03b5 \u03c4\u03b1 \u03bd\u03b5\u03c1\u03ac \u03ba\u03ac\u03c0\u03bf\u03b9\u03bf\u03c5 \u03b4\u03b5\u03bd \u03b4\u03bf\u03cd\u03bb\u03b5\u03c8\u03b5, you're capable). Also trust your gut, same thing as in interviews. this is a very long-term project, it therefore transcends individuals - as we've already stressed, always always always think about the long-term have a plan B, C, D, ... (even about design, even about people (just need it to work - Akin's Law)) it's a marathon, not a sprint - a phrase of a friend that I feel encapsulates all of this well (... but there will be sprints) bridge: TODO: long term and patterns and things you must keep in mind People and problems :( always find the next gen, preemptively. Otherwise it will be too late and you'll be patching holes. We've had that problem multiple times more often than not, there's a bigger underlying problem, and it has to do with structure. People are usually not the source of the problem - don't be too eager to blame something on an individual, dig deeper but people CAN be the source of the problem. Never underestimate that. Especially if they are in a position of power, either organizationally or due to technical expertise don't make someone coo lightly (i.e. don't endow with power lightly). We've had that problem happen way too many times. It's better to leave a gap than to shoot yourself in the foot this way People management \u00b6 People can also be tough Again, there are a lot of difficulties you'll face when you're trying to get people to collaborate to see this through. There's some similar questions to ask yourselves. The first is how do you find someone to do the boring work? There will be a lot of it, filling spreadsheets, searching for manuals... Especially if you have to hold events and have a social presence too it's a demanding project without rewards (monetary, ECTS...) - how do you get people to do volunteering work? Because they want to do it. Therefore, you have to make sure they want to do it a lot How do you get ordinary people to do the extraordinary? You'll have to face great technical challenges while at the same time, constantly support and get the others pumped, be there to mediate in case any conflicts occur, etc. taking initiatives is vital - that has been one of our biggest complaints about members. There's new ideas someone has to think, and there's work that no one will do unless someone decides to take it up by themselves. It also helps tremendously in fostering an environment where people feel their peers are motivated how do you ensure meeting participation? HR - remember that you should think about how you approach these things too. A good example and place to start might be \"Leadership That Gets Results\" by Daniel Goleman in HBR. This is recommended for you to see that there's a lot of thought that can go behind things (TODO the report about collective meetings) and a lot of different ways you can approach someone physical presence is vital expand beyond the scope of the project, hang out, do cool things on the side - This team was to me something way way more than just the project. I've learned a ton, met cool people, made friends, found out what I want to do academically, etc. And this is what's kept me working in the project for so long ;) the project requires heavy investment. This means it will overlap with other areas of the member's lives. You have to take that into account and learn about other areas of their life in order to effectively support them. This goes hand in hand with forming groups and being part of a gang Talk! ALWAYS face your problems, always speak, always communicate - hoarding problems under the rag instead of facing them is a very well known human tendency. I find that the more long term the thing you're not doing is, the more catastrophic the consequences will be when the time comes to face them. Again, this sounds like a cliche, \"just do everything in time\" so I will give you an example (TODO: mention EPS) communicate problems to ESA - again, they're hear to help. The sooner the learn about something the better. They'll always appreciate you being upfront, trust me. Don't put yourselves into rabbit holes you'll then have to dig out of feedback is crucial, it's all about feedback cycles - This is by far the most important think to take out of this talk. Feedback requires good communication channels, so first build these. Then learn to give feedback. Learn to receive feedback. Provide people with a lot of different ways they can give feedback. Make sure that there's always more than one person (that hopefully occupies a different position in the team) for someone to speak to. Feedback cycles is the way we get better. Again, make sure to do everything you can to have people a) giving feedback and b) receiving feedback and acting on it praise in public blame in private (not always) - a good reminder to keep in mind mostly the praising The leadership behind the leadership set up a \"core\" team of people to serve as the backbone of the team at all times - This is, organizationally, what helped us make it. It also affected our progress severely when we lacked it who thinks about the team and not just technical work or events? - That's how you can find the people you'll rely upon decisions will always upset some people. The question is which - If all goes south, side with the ones that will carry the team on their shoulders. They're your most important asset. Always consider how a decision might affect them and their motivation invest in those you believe will carry the team forward, not from a technical viewpoint this might mean not going with the majority. The majority will also blindly follow whatever, it's a formless mass to give shape to (oops) don't shadow-government much - this all might go against transparency and people will eventually catch up to that. Be transparent almost always Help them help you people won't do things they don't want to do forever - never forget that, try to find an alternative there's always someone out there that wants to work on what you don't. Find them newer members can find something more exciting, resource allocation also talk about the chef tasks book (\"Work Clean\"), you should also include things in feedback on how to work more efficiently, etc. Help them help you: Vol 2 don't leave anyone on their own set group tasks set small, well-defined tasks. Make the members feel and visualize the progress make sure everyone is on the same page regarding the high-level roadmap give people things they might not love but that are important, and things that aren't important but they love doing - try to find asteris top model progress is seldom linear, or apparent. Try to make it apparent, but have that in mind involve people and throw problems at them (but always be there when needed) - don't be afraid to throw problems at them, that's how they grow Fluff Some other things I wanted to make time to mention but couldn't group with the rest of the topics sometimes you have to be quick to arrive at a decision. That might mean you won't have time to get everyone to agree with said decision recruitment is key. You'll also need to establish presence in the uni(s) to get people in REC Akin's Laws - HEH TODO: talk about the importance of having a vision, of having some pillars, of your work being multifaceted and extending past just some research topic, etc. Mention our mission as an example and explain why that's so so so important (e.g. to screenshot apo random) Miniaturization of cell culturing instrumentation \u00b6 Ensuring biocompatibility of payload materials \u00b6 Regulating temperature and pressure to decouple cell survival-behavior from other space stressors (e.g. radiation) \u00b6 Using methods for autonomous measurements (e.g. spectroscopy, microscopy) \u00b6 Preparing predictable long-term biological sample storage methods before launch \u00b6 Engineering reliable readouts for cell viability/metabolism \u00b6 Standardization of common wet lab methods to meet space engineering requirements/procedures \u00b6","title":"ESA Design Booster"},{"location":"Projects/ESA%20Design%20Booster/#introduction","text":"Greetings! I'm Orestis, I come from Greece and I'm the co-science lead in the AcubeSAT project. AcubeSAT is a project of a student team called SpaceDot and is part of the Fly Your Satellite 3! program. I've been involved in this effort since almost the very beginnings, when everything was still at a nascent stage. I'm currently getting to wrap up my studies, and I've joined the team not long after I enrolled in the university, I've worked in different aspects of the project, technical and not, and now I'm responsible for the Science-y stuff. The AcubeSAT nanosatellite undertaking began late 2018 - 2019. We've designed and are developing a 3U CubeSat which holds a 2U biology payload. Our mission is two-fold: we want to establish our idea for a modular platform to perform space biology experiments in CubeSats/small satellites as a working prototype. Also, we aim to probe the way conditions in LEO (mainly microgravity and cosmic radiation) affect yeast cells at the gene expression level. To achieve that, the 2U payload is a pressurized vessel which hosts a container with the various compartments to run our experiments. We'll culture cells in-orbit and then see how their gene expression is altered through acquiring images via our DIY microscope-like imaging system. We aim to run the same experiment in three distinct timepoints across the duration of our mission, and to do this we've been using a small platform called a LoC to hold the cells and interface them with the various fluidics in a miniaturized version of a biology lab; but more on that at the end. In brief, we've submitted our proposal in October 2019, took part in the selection workshop hosted here in December 2019, got accepted in February 2020, submitted the TS-VCD (requirements) in April, submitted the first version of CDR in October and had it approved in September 2021, marking the end of the design phase and the beginning of the construction/testing phase. Right now we are getting ready to run a series of testing campaigns at the ESA facilities, then pass the MRR and be in-orbit by late 2023/early 2024.","title":"Introduction"},{"location":"Projects/ESA%20Design%20Booster/#context","text":"Before getting on with the actual presentation, I feel the need to underline the overall surrounding context regarding the team I've been and am a part of and the general environment in which we've been undertaking the FYS! journey. Maybe some of these will align with the environment and the challenges you are to face, maybe not. 1) There's no significant activity in the Aerospace sector in Greece, with the exception of some little companies and organizations that are trying to kickstart things, but we're still at a very early stage 2) On a similar manner, there's not much related expertise or a degree (e.g. in Aerospace engineering) to be offered in Greek universities. These mean it's difficult to get financial or technical support for the project 3) Student's don't gain ECTS credits etc. for being involved in the project to count towards completing a degree. People usually don't get to work at something space-related afterwards, at least in Greece, most enter the team stay for 1 - 1.5 year and then leave","title":"Context"},{"location":"Projects/ESA%20Design%20Booster/#presentation-scope","text":"The presentation is focused on discussing some of the programmatic aspects of the challenges you might come to face as FYS! progresses, instead of getting into the nitty gritty things on the technical side. Because I wanted to cover a lot of different topics in a short amount of time, it will be a little free flowing.","title":"Presentation scope"},{"location":"Projects/ESA%20Design%20Booster/#presentation-structure","text":"The presentation is split into topics that fall mainly into two distinct categories, the first being project management, while the second follows a more people-centric narrative. There will be advice with an emphasis on the earlier stages of the project, and some good and... not that good decisions to make or avoid, respectively. I'll try cast a more personal light and share some hurdles we've faced and had to overcome, or that we are still facing to this day. Following that tangent, I'll briefly go over some particular examples that greatly affected our team. Then, I'll mention some additional tips, and will close the presentation with an addendum on trying to realize a mission carrying a biological payload, since I've been informed there are some teams interested in working towards that. Because this is somewhat of a niche, please feel free to come have a chat with me after the Q&A session is over, so that we can talk in more detail. I'll share some handles and ways you can reach out to me, I and the whole team are very open and eager to discuss and chime in in whichever way possible.","title":"Presentation Structure"},{"location":"Projects/ESA%20Design%20Booster/#project-management","text":"there's always exceptions to rules. Understand the rules, then break them - don't take what I say as a hard absolute to be followed. It's empirical data gathered through observation. Think about what I say and try to understand why I say it, throughout the talk Things are tough you have to develop skillset from zero - not just technical, but mostly the organizational almost everything is behind closed doors, you don't even know what's out there, you have to learn but how? you have to find support (e.g. from university) you have to ensure infrastructure (e.g. machines) - might be something very generic to get prototypes, might be something very specific related to your payload. Getting access to required infrastructure might be almost trivial in some cases, near impossible in others. You'll face this sooner or later, start thinking about it as early as possible invest in open-source and the community. There's people you can learn from, if not teach - talk a little bit about open-source and how it has helped us and others. In any case, do some research for what's out there. FYS! is about CubeSats. CubeSats are very affordable and easy to build and verify, which means in turn that they are very approachable and that a lot of people who are somehow involved with a similar mission do not belong in the space industry, meaning there's a wealth of information that isn't as strictly regulated you got help from ESA, experts (esp. the experts), reach out - a lot of experience that you can't find elsewhere. Don't be afraid to ask because it might seem like you haven't done your research; it's an educational program and they want you to succeed. Remember that they've went through the whole journey multiple times. This is very crucial to take advantage of as early as possible. They won't just help you solve problems, they might give you fundamentally new ideas and paths to explore other teams participate in FYS!; be in touch - in other FYS! versions and other related projects, we've helped and received helped bridge: you can get help, and you will need help, because: Scale up there's no leadership voice outside of the team, the organization has to be DIY - biggest factor towards your success, no one will help you, and you won't have expertise. It's one thing to learn how to blink LEDs, and another to learn how to run a team of 40 people to meet strict deadlines in a very demanding project COVID hit, and it hit hard. Why? - a good segue, we had to rethink our organizational structure due to limited physical access. A hybrid scheme works best you'll eventually have to scale up, time and time again. How? - your organization must not only stand the test of \"now\", but also be future proof find good infrastructure, platforms, esp. for organization - can't stress this enough. Communication, tracking and finding information, project planning and management. Don't be pushing things back because you have a deadline to meet. Don't be afraid to invest early, plant the seeds; they will bloom later. Information transfer try maintain information transfer at all costs - our biggest problem documentation is always needed after completion (Akin's law), it almost never gets written post. If you come to me after I announce that I'll leave and say hey, can you write documentation and/or train new members? It will never happen. It will be sub-optimal at best bridge: TODO: organization Organization: the small keep minutes, have agendas, don't recycle topics, end each topic with an action, make the most out of a meeting cross-team meetings are very important hold concurrent sessions, frequent meetings, be transparent always bridge: on a more general note, ... ... and the big you need SYE, aka systems overview: for management, always see the forest, not the tree - how do all the pieces interact with all the other pieces? always be wary of cross-dependencies - some work that needs to be done needs other work to be done first; or I'm needed at A but I'm also needed at B constantly track things, move deadlines appropriately, be on top of things - you'll come to find out that you'll be setting a deadline and it will be way overdue and then you'll have to set a deadline anew and do that because it helps you know where you are and where you need to at all times be agile, especially in the beginning. Find what's good for you. Understand why it's good for you - purposely talking in a loose frame and giving general comments, self-reflection is the most important thing bridge: exploring a path that you might later on decide that you won't follow, playing with the design, the structure, being bold. You have time, you can afford to make mistakes Calm down you'll screw up, constantly. Relax and learn from it - it sounds very cliche, but it holds so so true. We've had so many situations where things almost derailed and came close to crumbling down because people were losing their minds. Relax, be calm, assess the situation, do your best, learn from any mistakes and move on. In the grand scheme of things, it probably doesn't matter that much diffuse situations, avoid crowd mentality - we've had \"critical\" situations dozens of times, we're still here remember, everything's gonna be alright if it isn't, not to worry. It's an educational program - I'm not trying to set the bar low here, things are very challenging and you should strive for excellence, but you have to understand the nature of the program and that the ESA people are here to help and that they know you will commit a lot of mistakes. The thought that they expect you to make mistakes is very liberating, because you will make mistakes and you might think that the consequences will be way bigger than they actually will be. I've noticed that they even get anxious if time passes and you don't seem to be having any problems. Plus, having a problem is great because you know that something is wrong, you probably know what it is, and you will eventually figure out a way to fix things. That's way better than sitting awkwardly wondering whether there's a problem looming around the corner waiting to bite you when you're not looking try to build a momentum, expect a momentum to be built, then ride it, but guide it too; don't let it carry you - I keep saying that things are difficult; you'll find it difficult at first with a lot of things, for example with the organizational structure. It will require a lot of effort in the beginning, it's like a slope where, if you're doing it right, little by little it will get easier with time. Our case happened after we delivered the first CDR version successfully, things somewhat got automated, everyone knew what to work on, communication was frictionless, new members were seamlessly integrated, but you have to always keep things in check and revisit things. We did this mistake and cracks started to appear; it costs us a lot in the end. The bus factor be afraid of the bus factor . The bus factor is when ... and you will come across it and some times there won't be a lot of things you'll be able to do to mitigate it. But also always remember: NO ONE is irreplaceable - (\u03cc\u03c3\u03b5\u03c2 \u03c6\u03bf\u03c1\u03ad\u03c2 \u03b5\u03af\u03c0\u03b1\u03bc\u03b5 \u03bd\u03b1 \u03c0\u03ac\u03bc\u03b5 \u03bc\u03b5 \u03c4\u03b1 \u03bd\u03b5\u03c1\u03ac \u03ba\u03ac\u03c0\u03bf\u03b9\u03bf\u03c5 \u03b4\u03b5\u03bd \u03b4\u03bf\u03cd\u03bb\u03b5\u03c8\u03b5, you're capable). Also trust your gut, same thing as in interviews. this is a very long-term project, it therefore transcends individuals - as we've already stressed, always always always think about the long-term have a plan B, C, D, ... (even about design, even about people (just need it to work - Akin's Law)) it's a marathon, not a sprint - a phrase of a friend that I feel encapsulates all of this well (... but there will be sprints) bridge: TODO: long term and patterns and things you must keep in mind People and problems :( always find the next gen, preemptively. Otherwise it will be too late and you'll be patching holes. We've had that problem multiple times more often than not, there's a bigger underlying problem, and it has to do with structure. People are usually not the source of the problem - don't be too eager to blame something on an individual, dig deeper but people CAN be the source of the problem. Never underestimate that. Especially if they are in a position of power, either organizationally or due to technical expertise don't make someone coo lightly (i.e. don't endow with power lightly). We've had that problem happen way too many times. It's better to leave a gap than to shoot yourself in the foot this way","title":"Project management"},{"location":"Projects/ESA%20Design%20Booster/#people-management","text":"People can also be tough Again, there are a lot of difficulties you'll face when you're trying to get people to collaborate to see this through. There's some similar questions to ask yourselves. The first is how do you find someone to do the boring work? There will be a lot of it, filling spreadsheets, searching for manuals... Especially if you have to hold events and have a social presence too it's a demanding project without rewards (monetary, ECTS...) - how do you get people to do volunteering work? Because they want to do it. Therefore, you have to make sure they want to do it a lot How do you get ordinary people to do the extraordinary? You'll have to face great technical challenges while at the same time, constantly support and get the others pumped, be there to mediate in case any conflicts occur, etc. taking initiatives is vital - that has been one of our biggest complaints about members. There's new ideas someone has to think, and there's work that no one will do unless someone decides to take it up by themselves. It also helps tremendously in fostering an environment where people feel their peers are motivated how do you ensure meeting participation? HR - remember that you should think about how you approach these things too. A good example and place to start might be \"Leadership That Gets Results\" by Daniel Goleman in HBR. This is recommended for you to see that there's a lot of thought that can go behind things (TODO the report about collective meetings) and a lot of different ways you can approach someone physical presence is vital expand beyond the scope of the project, hang out, do cool things on the side - This team was to me something way way more than just the project. I've learned a ton, met cool people, made friends, found out what I want to do academically, etc. And this is what's kept me working in the project for so long ;) the project requires heavy investment. This means it will overlap with other areas of the member's lives. You have to take that into account and learn about other areas of their life in order to effectively support them. This goes hand in hand with forming groups and being part of a gang Talk! ALWAYS face your problems, always speak, always communicate - hoarding problems under the rag instead of facing them is a very well known human tendency. I find that the more long term the thing you're not doing is, the more catastrophic the consequences will be when the time comes to face them. Again, this sounds like a cliche, \"just do everything in time\" so I will give you an example (TODO: mention EPS) communicate problems to ESA - again, they're hear to help. The sooner the learn about something the better. They'll always appreciate you being upfront, trust me. Don't put yourselves into rabbit holes you'll then have to dig out of feedback is crucial, it's all about feedback cycles - This is by far the most important think to take out of this talk. Feedback requires good communication channels, so first build these. Then learn to give feedback. Learn to receive feedback. Provide people with a lot of different ways they can give feedback. Make sure that there's always more than one person (that hopefully occupies a different position in the team) for someone to speak to. Feedback cycles is the way we get better. Again, make sure to do everything you can to have people a) giving feedback and b) receiving feedback and acting on it praise in public blame in private (not always) - a good reminder to keep in mind mostly the praising The leadership behind the leadership set up a \"core\" team of people to serve as the backbone of the team at all times - This is, organizationally, what helped us make it. It also affected our progress severely when we lacked it who thinks about the team and not just technical work or events? - That's how you can find the people you'll rely upon decisions will always upset some people. The question is which - If all goes south, side with the ones that will carry the team on their shoulders. They're your most important asset. Always consider how a decision might affect them and their motivation invest in those you believe will carry the team forward, not from a technical viewpoint this might mean not going with the majority. The majority will also blindly follow whatever, it's a formless mass to give shape to (oops) don't shadow-government much - this all might go against transparency and people will eventually catch up to that. Be transparent almost always Help them help you people won't do things they don't want to do forever - never forget that, try to find an alternative there's always someone out there that wants to work on what you don't. Find them newer members can find something more exciting, resource allocation also talk about the chef tasks book (\"Work Clean\"), you should also include things in feedback on how to work more efficiently, etc. Help them help you: Vol 2 don't leave anyone on their own set group tasks set small, well-defined tasks. Make the members feel and visualize the progress make sure everyone is on the same page regarding the high-level roadmap give people things they might not love but that are important, and things that aren't important but they love doing - try to find asteris top model progress is seldom linear, or apparent. Try to make it apparent, but have that in mind involve people and throw problems at them (but always be there when needed) - don't be afraid to throw problems at them, that's how they grow Fluff Some other things I wanted to make time to mention but couldn't group with the rest of the topics sometimes you have to be quick to arrive at a decision. That might mean you won't have time to get everyone to agree with said decision recruitment is key. You'll also need to establish presence in the uni(s) to get people in REC Akin's Laws - HEH TODO: talk about the importance of having a vision, of having some pillars, of your work being multifaceted and extending past just some research topic, etc. Mention our mission as an example and explain why that's so so so important (e.g. to screenshot apo random)","title":"People management"},{"location":"Projects/ESA%20Design%20Booster/#miniaturization-of-cell-culturing-instrumentation","text":"","title":"Miniaturization of cell culturing instrumentation"},{"location":"Projects/ESA%20Design%20Booster/#ensuring-biocompatibility-of-payload-materials","text":"","title":"Ensuring biocompatibility of payload materials"},{"location":"Projects/ESA%20Design%20Booster/#regulating-temperature-and-pressure-to-decouple-cell-survival-behavior-from-other-space-stressors-eg-radiation","text":"","title":"Regulating temperature and pressure to decouple cell survival-behavior from other space stressors (e.g. radiation)"},{"location":"Projects/ESA%20Design%20Booster/#using-methods-for-autonomous-measurements-eg-spectroscopy-microscopy","text":"","title":"Using methods for autonomous measurements (e.g. spectroscopy, microscopy)"},{"location":"Projects/ESA%20Design%20Booster/#preparing-predictable-long-term-biological-sample-storage-methods-before-launch","text":"","title":"Preparing predictable long-term biological sample storage methods before launch"},{"location":"Projects/ESA%20Design%20Booster/#engineering-reliable-readouts-for-cell-viabilitymetabolism","text":"","title":"Engineering reliable readouts for cell viability/metabolism"},{"location":"Projects/ESA%20Design%20Booster/#standardization-of-common-wet-lab-methods-to-meet-space-engineering-requirementsprocedures","text":"","title":"Standardization of common wet lab methods to meet space engineering requirements/procedures"},{"location":"Projects/GitHub/African%20Buffalo%20Optimization%20%28ABO%29/","text":"Introduction \u00b6 Inspired by the migrant behavior of African buffalos in the vast African landscape. African buffalos organize their large herds using only two principal sounds. They navigate the landscape from point A to point B in search of pasture. They even watched National Geographic documentaries :O Advantages: fast, robust, effective, efficient, simple to implement/user-friendly; good ability to explore and exploit the search space. Addendum: How to develop a swarm intelligence algorithm \u00b6 Observe behavior of group of organisms and note that it realizes objectives impossible for any individual group member Model the behavior Develop a mathematical model based on the behavior model Write pseudocode Implement in code Parameter tuning through mathematical and experimental analysis Flowchart \u00b6 Initialize the buffalos within the search space. Calculate the buffalo exploitation: $m_k' = m_k + lp_1(bg - w_k) + lp_2(bp_k - w_k)$ Calculate the buffalo locations: $w_k' = \\frac{(w_k + m_k)}{\\lambda}$ Determine if $bg$ is updating. If yes, proceed to $5$. Else, proceed to $2$. Crosscheck stopping criteria. If reached, proceed to $6$. Else, proceed to $2$. Output best solution. Here, $w_k$ is the /waaa/ call, which mobilizes the herd to move (explore) with reference to buffalo $k$. $m_k$ is the /maaa/ call, which calls to exploit. $w'_k$ is a call for more exploration, while $m_k'$ a call for more exploitation, respectively. $bg$ is the buffalo with the best position in relation to the global optimum. $bp_k$ is the buffalo's personal best position. $lp_1$ and $lp_2$ are the so-called learning parameters. $\\lambda$ is a random number, called the exploration driver, which takes any value between $0$ and $1$, depending on the problem being solved; the higher $\\lambda$ is, we get more exploitation and less exploration, and vice-versa. Math \u00b6 First, randomly initialize the buffalo population within the search space. Next, evaluate buffalo exploitation capacities using $(1)$. This is needed to determine the next move the buffalos are going to take in their search for pastures. $(1)$ is \"democratic\" and is fed to $(2)$, which describes exploration. This determines whether the buffalo herd will stay in the same location, or whether it will migrate to another location. If the \"best\" buffalo $bg$ is updating, check whether the stopping criteria have been met. If so, the search has been concluded. Otherwise, go back and reassess the buffalo exploitation values. Equation $(1)$ is the controlling equation, which propels the entire herd to relocate, on the basis that the new location will probably be more rewarding than the current one: $$m_k' = m_k + lp_1(bg - w_k) + lp_2(bp_k - w_k) \\tag{1}$$ Equation $(1)$ has three parts. First, the memory part ($m_k'$), which reminds the buffalos that they have relocated to a new location from the previous location ($m_k$). Second, the part which signals the cooperative behavior of the buffalos ($lp_1 (bg - w_k)$). Third, part which represents the buffalos capacity for communication across the entire herd ($lp_2(bp_k - w_k)$). The last (third) part of $(1)$, $lp_2(bp_k - w_k)$, underscores the buffalos intelligence. They can tell which their previous best location was, compared with their current location. This enables them to retrace their steps to the best, to their knowledge, location, which is extremely useful if they stray away towards a starving location. To summarize, equation $(1)$ describes the buffalo memory, ability to harness collective intelligence, and intra-herd communication. $lp_1$ and $lp_2$ undergo parameter tuning, depending on the problem context. A higher value of $lp_1$ biases the algorithm toward global search while a higher $lp_2$ biases towards a local search. Equation $(2)$ is what moves the buffalos to the new location, following the decision taken through $(1)$: $$w_k' = \\frac{(w_k + m_k)}{\\lambda} \\tag{2}$$ $(2)$ shows that the movement of the buffalos is a function of the /waaa/ ($w_k$) and the /maaa/ ($m_k$) calls. It's moderated by the exploration driver, $\\lambda$, which takes a value between $0$ and $1$. So, the movement of buffalo $k$ from $w_k$ (present exploration location) to other locations is influenced by $m_k$, the exploitation location, and adjusting its position appropriately in relation to the herd's best $(bg - w_k)$ as well as its personal best ($bp_k - w_k$) with the bias of the learning parameters. Application in 2D \u00b6 References \u00b6 @odiliStochasticProcessTutorial2022","title":"African Buffalo Optimization (ABO)"},{"location":"Projects/GitHub/African%20Buffalo%20Optimization%20%28ABO%29/#introduction","text":"Inspired by the migrant behavior of African buffalos in the vast African landscape. African buffalos organize their large herds using only two principal sounds. They navigate the landscape from point A to point B in search of pasture. They even watched National Geographic documentaries :O Advantages: fast, robust, effective, efficient, simple to implement/user-friendly; good ability to explore and exploit the search space.","title":"Introduction"},{"location":"Projects/GitHub/African%20Buffalo%20Optimization%20%28ABO%29/#addendum-how-to-develop-a-swarm-intelligence-algorithm","text":"Observe behavior of group of organisms and note that it realizes objectives impossible for any individual group member Model the behavior Develop a mathematical model based on the behavior model Write pseudocode Implement in code Parameter tuning through mathematical and experimental analysis","title":"Addendum: How to develop a swarm intelligence algorithm"},{"location":"Projects/GitHub/African%20Buffalo%20Optimization%20%28ABO%29/#flowchart","text":"Initialize the buffalos within the search space. Calculate the buffalo exploitation: $m_k' = m_k + lp_1(bg - w_k) + lp_2(bp_k - w_k)$ Calculate the buffalo locations: $w_k' = \\frac{(w_k + m_k)}{\\lambda}$ Determine if $bg$ is updating. If yes, proceed to $5$. Else, proceed to $2$. Crosscheck stopping criteria. If reached, proceed to $6$. Else, proceed to $2$. Output best solution. Here, $w_k$ is the /waaa/ call, which mobilizes the herd to move (explore) with reference to buffalo $k$. $m_k$ is the /maaa/ call, which calls to exploit. $w'_k$ is a call for more exploration, while $m_k'$ a call for more exploitation, respectively. $bg$ is the buffalo with the best position in relation to the global optimum. $bp_k$ is the buffalo's personal best position. $lp_1$ and $lp_2$ are the so-called learning parameters. $\\lambda$ is a random number, called the exploration driver, which takes any value between $0$ and $1$, depending on the problem being solved; the higher $\\lambda$ is, we get more exploitation and less exploration, and vice-versa.","title":"Flowchart"},{"location":"Projects/GitHub/African%20Buffalo%20Optimization%20%28ABO%29/#math","text":"First, randomly initialize the buffalo population within the search space. Next, evaluate buffalo exploitation capacities using $(1)$. This is needed to determine the next move the buffalos are going to take in their search for pastures. $(1)$ is \"democratic\" and is fed to $(2)$, which describes exploration. This determines whether the buffalo herd will stay in the same location, or whether it will migrate to another location. If the \"best\" buffalo $bg$ is updating, check whether the stopping criteria have been met. If so, the search has been concluded. Otherwise, go back and reassess the buffalo exploitation values. Equation $(1)$ is the controlling equation, which propels the entire herd to relocate, on the basis that the new location will probably be more rewarding than the current one: $$m_k' = m_k + lp_1(bg - w_k) + lp_2(bp_k - w_k) \\tag{1}$$ Equation $(1)$ has three parts. First, the memory part ($m_k'$), which reminds the buffalos that they have relocated to a new location from the previous location ($m_k$). Second, the part which signals the cooperative behavior of the buffalos ($lp_1 (bg - w_k)$). Third, part which represents the buffalos capacity for communication across the entire herd ($lp_2(bp_k - w_k)$). The last (third) part of $(1)$, $lp_2(bp_k - w_k)$, underscores the buffalos intelligence. They can tell which their previous best location was, compared with their current location. This enables them to retrace their steps to the best, to their knowledge, location, which is extremely useful if they stray away towards a starving location. To summarize, equation $(1)$ describes the buffalo memory, ability to harness collective intelligence, and intra-herd communication. $lp_1$ and $lp_2$ undergo parameter tuning, depending on the problem context. A higher value of $lp_1$ biases the algorithm toward global search while a higher $lp_2$ biases towards a local search. Equation $(2)$ is what moves the buffalos to the new location, following the decision taken through $(1)$: $$w_k' = \\frac{(w_k + m_k)}{\\lambda} \\tag{2}$$ $(2)$ shows that the movement of the buffalos is a function of the /waaa/ ($w_k$) and the /maaa/ ($m_k$) calls. It's moderated by the exploration driver, $\\lambda$, which takes a value between $0$ and $1$. So, the movement of buffalo $k$ from $w_k$ (present exploration location) to other locations is influenced by $m_k$, the exploitation location, and adjusting its position appropriately in relation to the herd's best $(bg - w_k)$ as well as its personal best ($bp_k - w_k$) with the bias of the learning parameters.","title":"Math"},{"location":"Projects/GitHub/African%20Buffalo%20Optimization%20%28ABO%29/#application-in-2d","text":"","title":"Application in 2D"},{"location":"Projects/GitHub/African%20Buffalo%20Optimization%20%28ABO%29/#references","text":"@odiliStochasticProcessTutorial2022","title":"References"},{"location":"Projects/GitHub/Summation%20By%20Parts%20%28SBP%29/","text":"From @mattssonBoundaryOptimizedDiagonalnorm2018 : The Matlab functions return the differentiation matrix D1, the norm matrix H , the grid point vector x, and the interior grid spacing h. Input to the functions are the grid size m and the width L of the domain. ( m is actually N in code): function [D1,H,x,h] = D1_accurate_8th ( N,L ) Variable Definition D1 Differentiation matrix H Norm matrix x Grid (non-equidistant) point vector h Interior grid spacing m Grid size L Domain length BP Number of boundary points order Accuracy of interior stencil Julia Implementation \u00b6 Split between /dev/MattssonAlmquistVanDerWeide2018... and /src/MattssonAlmquistVanDerWeide2018... . $x_k$, the non-equidistant grid points in MATLAB ( D1_accurate_8th.m ): %%%% Non-equidistant grid points %%%%% x0 = 0.0000000000000e+00 ; x1 = 3.8118550247622e-01 ; x2 = 1.1899550868338e+00 ; x3 = 2.2476300175641e+00 ; x4 = 3.3192851303204e+00 ; x5 = 4.3192851303204e+00 ; x6 = 5.3192851303204e+00 ; x7 = 6.3192851303204e+00 ; x8 = 7.3192851303204e+00 ; Reside in /src/ : elseif accuracy_order == 8 xstart = SVector ( T ( 0.0000000000000e+00 ), T ( 3.8118550247622e-01 ), T ( 1.1899550868338e+00 ), T ( 2.2476300175641e+00 ), T ( 3.3192851303204e+00 ), T ( 4.3192851303204e+00 ), T ( 5.3192851303204e+00 ), T ( 6.3192851303204e+00 ), T ( 7.3192851303204e+00 ), ) There is a mention in @mattssonBoundaryOptimizedDiagonalnorm2018 about the $Q$ matrix: Definition 2.2: A difference operator $D_1 = H^{-1}Q$, approximating $\\partial/\\partial x$ using a repeated narrow-stencil in the interior, is said to be a diagonal-norm first-derivative SBP operator if $H$ is diagonal and positive definite, and $Q + Q^T = diag(-1, 0, \\ldots, 0, 1)$. The interior stencil of $Q$ is in MATLAB: switch order case 2 d = [ - 1 / 2 , 0 , 1 / 2 ]; case 4 d = [ 1 / 12 , - 2 / 3 , 0 , 2 / 3 , - 1 / 12 ]; case 6 d = [ - 1 / 60 , 3 / 20 , - 3 / 4 , 0 , 3 / 4 , - 3 / 20 , 1 / 60 ]; case 8 d = [ 1 / 280 , - 4 / 105 , 1 / 5 , - 4 / 5 , 0 , 4 / 5 , - 1 / 5 , 4 / 105 , - 1 / 280 ]; case 10 d = [ - 1 / 1260 , 5 / 504 , - 5 / 84 , 5 / 21 , - 5 / 6 , 0 , 5 / 6 , - 5 / 21 , 5 / 84 , - 5 / 504 , 1 / 1260 ]; case 12 d = [ 1 / 5544 , - 1 / 385 , 1 / 56 , - 5 / 63 , 15 / 56 , - 6 / 7 , 0 , 6 / 7 , - 15 / 56 , 5 / 63 , - 1 / 56 , 1 / 385 , - 1 / 5544 ]; end For, e.g. order 8, the relevant part is case 8 d = [ 1 / 280 , - 4 / 105 , 1 / 5 , - 4 / 5 , 0 , 4 / 5 , - 1 / 5 , 4 / 105 , - 1 / 280 ]; which can be found in src/ : upper_coef = SVector ( T ( 4 // 5 ), T ( - 1 // 5 ), T ( 4 // 105 ), T ( - 1 // 280 )) lower_coef = - upper_coef For the norm matrix, we have: P0 = 1.0758368078310e-01 ; P1 = 6.1909685107891e-01 ; P2 = 9.6971176519117e-01 ; P3 = 1.1023441350947e+00 ; P4 = 1.0244688965833e+00 ; P5 = 9.9533550116831e-01 ; P6 = 1.0008236941028e+00 ; P7 = 9.9992060631812e-01 ; which can be found in dev/ : h = [ 1.0758368078310e-01 , 6.1909685107891e-01 , 9.6971176519117e-01 , 1.1023441350947e+00 , 1.0244688965833e+00 , 9.9533550116831e-01 , 1.0008236941028e+00 , 9.9992060631812e-01 , 1 , 1 , 1 , 1 ] question : what about the trailing ones? TODO: how do I determine how many to add? The boundaries, $Q_k$ are: ... Q0_0 = - 5.0000000000000e-01 ; Q0_1 = 6.7284756079369e-01 ; Q0_2 = - 2.5969732837062e-01 ; Q0_3 = 1.3519390385721e-01 ; Q0_4 = - 6.9678474730984e-02 ; Q0_5 = 2.6434024071371e-02 ; Q0_6 = - 5.5992311465618e-03 ; Q0_7 = 4.9954552590464e-04 ; Q0_8 = 0.0000000000000e+00 ; Q0_9 = 0.0000000000000e+00 ; Q0_10 = 0.0000000000000e+00 ; Q0_11 = 0.0000000000000e+00 ; which also reside in dev/ : julia q1 = [0, 6.7284756079369e-01, -2.5969732837062e-01, 1.3519390385721e-01, -6.9678474730984e-02, 2.6434024071371e-02, -5.5992311465618e-03, 4.9954552590464e-04, 0, 0, 0, 0]' question : in this example, why is the original Q0_0 = -5.0000000000000e-01; , while it's 0 in the Julia implementation? question : why D1 = H \\ (Q - 1//2*e*e') to construct the difference operator $D1$? question : I understand that dev/ is used first to get the values used in src/ , e.g. for the DerivativeCoefficientRow s. How? dev is just the authors' set of notes while translating, the real coefficients live in src For the derivative operator, coefficients of D1 are used, not of Q . Thus, the author usually translated Q coefficients from a paper to D1 coefficients for this package The SBP property is M * D1 + D1' * M' = eR * eR' - eL * eL' with eL = [1, 0, ..., 0] and eR = [0, ..., 0, 1] . This allows to translate Q to D1 and vice-versa","title":"Summation By Parts (SBP)"},{"location":"Projects/GitHub/Summation%20By%20Parts%20%28SBP%29/#julia-implementation","text":"Split between /dev/MattssonAlmquistVanDerWeide2018... and /src/MattssonAlmquistVanDerWeide2018... . $x_k$, the non-equidistant grid points in MATLAB ( D1_accurate_8th.m ): %%%% Non-equidistant grid points %%%%% x0 = 0.0000000000000e+00 ; x1 = 3.8118550247622e-01 ; x2 = 1.1899550868338e+00 ; x3 = 2.2476300175641e+00 ; x4 = 3.3192851303204e+00 ; x5 = 4.3192851303204e+00 ; x6 = 5.3192851303204e+00 ; x7 = 6.3192851303204e+00 ; x8 = 7.3192851303204e+00 ; Reside in /src/ : elseif accuracy_order == 8 xstart = SVector ( T ( 0.0000000000000e+00 ), T ( 3.8118550247622e-01 ), T ( 1.1899550868338e+00 ), T ( 2.2476300175641e+00 ), T ( 3.3192851303204e+00 ), T ( 4.3192851303204e+00 ), T ( 5.3192851303204e+00 ), T ( 6.3192851303204e+00 ), T ( 7.3192851303204e+00 ), ) There is a mention in @mattssonBoundaryOptimizedDiagonalnorm2018 about the $Q$ matrix: Definition 2.2: A difference operator $D_1 = H^{-1}Q$, approximating $\\partial/\\partial x$ using a repeated narrow-stencil in the interior, is said to be a diagonal-norm first-derivative SBP operator if $H$ is diagonal and positive definite, and $Q + Q^T = diag(-1, 0, \\ldots, 0, 1)$. The interior stencil of $Q$ is in MATLAB: switch order case 2 d = [ - 1 / 2 , 0 , 1 / 2 ]; case 4 d = [ 1 / 12 , - 2 / 3 , 0 , 2 / 3 , - 1 / 12 ]; case 6 d = [ - 1 / 60 , 3 / 20 , - 3 / 4 , 0 , 3 / 4 , - 3 / 20 , 1 / 60 ]; case 8 d = [ 1 / 280 , - 4 / 105 , 1 / 5 , - 4 / 5 , 0 , 4 / 5 , - 1 / 5 , 4 / 105 , - 1 / 280 ]; case 10 d = [ - 1 / 1260 , 5 / 504 , - 5 / 84 , 5 / 21 , - 5 / 6 , 0 , 5 / 6 , - 5 / 21 , 5 / 84 , - 5 / 504 , 1 / 1260 ]; case 12 d = [ 1 / 5544 , - 1 / 385 , 1 / 56 , - 5 / 63 , 15 / 56 , - 6 / 7 , 0 , 6 / 7 , - 15 / 56 , 5 / 63 , - 1 / 56 , 1 / 385 , - 1 / 5544 ]; end For, e.g. order 8, the relevant part is case 8 d = [ 1 / 280 , - 4 / 105 , 1 / 5 , - 4 / 5 , 0 , 4 / 5 , - 1 / 5 , 4 / 105 , - 1 / 280 ]; which can be found in src/ : upper_coef = SVector ( T ( 4 // 5 ), T ( - 1 // 5 ), T ( 4 // 105 ), T ( - 1 // 280 )) lower_coef = - upper_coef For the norm matrix, we have: P0 = 1.0758368078310e-01 ; P1 = 6.1909685107891e-01 ; P2 = 9.6971176519117e-01 ; P3 = 1.1023441350947e+00 ; P4 = 1.0244688965833e+00 ; P5 = 9.9533550116831e-01 ; P6 = 1.0008236941028e+00 ; P7 = 9.9992060631812e-01 ; which can be found in dev/ : h = [ 1.0758368078310e-01 , 6.1909685107891e-01 , 9.6971176519117e-01 , 1.1023441350947e+00 , 1.0244688965833e+00 , 9.9533550116831e-01 , 1.0008236941028e+00 , 9.9992060631812e-01 , 1 , 1 , 1 , 1 ] question : what about the trailing ones? TODO: how do I determine how many to add? The boundaries, $Q_k$ are: ... Q0_0 = - 5.0000000000000e-01 ; Q0_1 = 6.7284756079369e-01 ; Q0_2 = - 2.5969732837062e-01 ; Q0_3 = 1.3519390385721e-01 ; Q0_4 = - 6.9678474730984e-02 ; Q0_5 = 2.6434024071371e-02 ; Q0_6 = - 5.5992311465618e-03 ; Q0_7 = 4.9954552590464e-04 ; Q0_8 = 0.0000000000000e+00 ; Q0_9 = 0.0000000000000e+00 ; Q0_10 = 0.0000000000000e+00 ; Q0_11 = 0.0000000000000e+00 ; which also reside in dev/ : julia q1 = [0, 6.7284756079369e-01, -2.5969732837062e-01, 1.3519390385721e-01, -6.9678474730984e-02, 2.6434024071371e-02, -5.5992311465618e-03, 4.9954552590464e-04, 0, 0, 0, 0]' question : in this example, why is the original Q0_0 = -5.0000000000000e-01; , while it's 0 in the Julia implementation? question : why D1 = H \\ (Q - 1//2*e*e') to construct the difference operator $D1$? question : I understand that dev/ is used first to get the values used in src/ , e.g. for the DerivativeCoefficientRow s. How? dev is just the authors' set of notes while translating, the real coefficients live in src For the derivative operator, coefficients of D1 are used, not of Q . Thus, the author usually translated Q coefficients from a paper to D1 coefficients for this package The SBP property is M * D1 + D1' * M' = eR * eR' - eL * eL' with eL = [1, 0, ..., 0] and eR = [0, ..., 0, 1] . This allows to translate Q to D1 and vice-versa","title":"Julia Implementation"},{"location":"Projects/Thesis/Tsitsas%20Email/","text":"Proposed Abstract \u00b6 Description \u00b6 Mathematics TL;DR: Model, optimization, etc. (technical stuff that he can immediately understand, abstracted from the actual application and everything bio) \u00b6 Bio introduction (ELI5) \u00b6 Actual bio introduction \u00b6 Mathematics \u00b6 From @ajo-franklinRationalDesignMemory2007 : We use a single mathematical framework to describe the relationship between the activator concentration and the production rate of either the reporter or autofeedback activator. We assume that the rate of production of either the reporter or autoactivator species, $(dX/dt)_{produce}$, can be described by the sum of a basal rate and a Hill function: $$\\begin{pmatrix}\\frac{dX}{dt}\\end{pmatrix} {produce} = s + \\frac{\\beta A^n}{K^n + A^n} \\tag{1}$$ where $s$ is the basal rate of production of activator, $n$ is the Hill cooperativity, $K$ is the concentration of activator that yields half the maximal production rate, $\\beta$ is the maximal rate of protein production, and $A$ is the activator consumption ( @alonIntroductionSystemsBiology2019 ). The decay rate of either the reporter or autoactivator concentration is affected by both dilution due to cell growth and protein degradation. However, since the reporter protein and the autoactivator are degraded at a rate that is many time slower than their dilution rate, the decay in the species concentration can be simplified to $$\\begin{pmatrix}\\frac{dX}{dt}\\end{pmatrix} = - \\frac{ln2}{\\tau}X \\tag{2}$$ where $\\tau$ is the time required for the number of cells to double by growth. The overall change in the concentration of the reporter or autofeedback activator can then be written as $$\\frac{dX}{dt} = s + \\frac{\\beta A^n}{K^n + A^n} - \\frac{ln2}{\\tau}X \\tag{3}$$ At steady state, the production rate is balanced by the decay rate, so the relationship between the steady-state activator concentration, $A$, and steady-state reporter concentration, $R$, can be written as $$R = \\frac{ln2}{\\tau}\\begin{pmatrix}s + \\frac{\\beta A^n}{K^n + A^n}\\end{pmatrix} \\tag{4}$$ In the case of the positive feedback loop, we assume that the autofeedback gene is activated identically by the sensor-derived activator and the autofeedback activator, so we can rewrite Equation $3$ as $$\\frac{dA_A}{dt} = s + \\frac{\\beta(A_A + A_s)^n}{K^n + (A_A + A_s)^n} - \\frac{ln2}{\\tau}A_A \\tag{5}$$ where $A_A$ is the autoactivator concentration and $A_s$ is the sensor-derived activator concentration. Figures \u00b6 Pair bio with mathematics \u00b6 Link to mission (SpaceDot - AcubeSAT) \u00b6 How will I go about doing this? ( tools , different from \"resources available\") \u00b6 Why I want to do this thesis \u00b6 What I want to do in my MSc (also mention that I want to build the somewhat more \"general\" skillset; also mention stochastic calculus, dyn. sys (+ differential geometry, manifolds etc.) and nonlinear, complex systems, chaos, just with the application I want) \u00b6 What resources I have at my disposal -> link with MSc (Asteris the friend) \u00b6 Proposed Roadmap \u00b6","title":"Tsitsas Email"},{"location":"Projects/Thesis/Tsitsas%20Email/#proposed-abstract","text":"","title":"Proposed Abstract"},{"location":"Projects/Thesis/Tsitsas%20Email/#description","text":"","title":"Description"},{"location":"Projects/Thesis/Tsitsas%20Email/#mathematics-tldr-model-optimization-etc-technical-stuff-that-he-can-immediately-understand-abstracted-from-the-actual-application-and-everything-bio","text":"","title":"Mathematics TL;DR: Model, optimization, etc. (technical stuff that he can immediately understand, abstracted from the actual application and everything bio)"},{"location":"Projects/Thesis/Tsitsas%20Email/#bio-introduction-eli5","text":"","title":"Bio introduction (ELI5)"},{"location":"Projects/Thesis/Tsitsas%20Email/#actual-bio-introduction","text":"","title":"Actual bio introduction"},{"location":"Projects/Thesis/Tsitsas%20Email/#mathematics","text":"From @ajo-franklinRationalDesignMemory2007 : We use a single mathematical framework to describe the relationship between the activator concentration and the production rate of either the reporter or autofeedback activator. We assume that the rate of production of either the reporter or autoactivator species, $(dX/dt)_{produce}$, can be described by the sum of a basal rate and a Hill function: $$\\begin{pmatrix}\\frac{dX}{dt}\\end{pmatrix} {produce} = s + \\frac{\\beta A^n}{K^n + A^n} \\tag{1}$$ where $s$ is the basal rate of production of activator, $n$ is the Hill cooperativity, $K$ is the concentration of activator that yields half the maximal production rate, $\\beta$ is the maximal rate of protein production, and $A$ is the activator consumption ( @alonIntroductionSystemsBiology2019 ). The decay rate of either the reporter or autoactivator concentration is affected by both dilution due to cell growth and protein degradation. However, since the reporter protein and the autoactivator are degraded at a rate that is many time slower than their dilution rate, the decay in the species concentration can be simplified to $$\\begin{pmatrix}\\frac{dX}{dt}\\end{pmatrix} = - \\frac{ln2}{\\tau}X \\tag{2}$$ where $\\tau$ is the time required for the number of cells to double by growth. The overall change in the concentration of the reporter or autofeedback activator can then be written as $$\\frac{dX}{dt} = s + \\frac{\\beta A^n}{K^n + A^n} - \\frac{ln2}{\\tau}X \\tag{3}$$ At steady state, the production rate is balanced by the decay rate, so the relationship between the steady-state activator concentration, $A$, and steady-state reporter concentration, $R$, can be written as $$R = \\frac{ln2}{\\tau}\\begin{pmatrix}s + \\frac{\\beta A^n}{K^n + A^n}\\end{pmatrix} \\tag{4}$$ In the case of the positive feedback loop, we assume that the autofeedback gene is activated identically by the sensor-derived activator and the autofeedback activator, so we can rewrite Equation $3$ as $$\\frac{dA_A}{dt} = s + \\frac{\\beta(A_A + A_s)^n}{K^n + (A_A + A_s)^n} - \\frac{ln2}{\\tau}A_A \\tag{5}$$ where $A_A$ is the autoactivator concentration and $A_s$ is the sensor-derived activator concentration.","title":"Mathematics"},{"location":"Projects/Thesis/Tsitsas%20Email/#figures","text":"","title":"Figures"},{"location":"Projects/Thesis/Tsitsas%20Email/#pair-bio-with-mathematics","text":"","title":"Pair bio with mathematics"},{"location":"Projects/Thesis/Tsitsas%20Email/#link-to-mission-spacedot-acubesat","text":"","title":"Link to mission (SpaceDot - AcubeSAT)"},{"location":"Projects/Thesis/Tsitsas%20Email/#how-will-i-go-about-doing-this-tools-different-from-resources-available","text":"","title":"How will I go about doing this? (tools, different from \"resources available\")"},{"location":"Projects/Thesis/Tsitsas%20Email/#why-i-want-to-do-this-thesis","text":"","title":"Why I want to do this thesis"},{"location":"Projects/Thesis/Tsitsas%20Email/#what-i-want-to-do-in-my-msc-also-mention-that-i-want-to-build-the-somewhat-more-general-skillset-also-mention-stochastic-calculus-dyn-sys-differential-geometry-manifolds-etc-and-nonlinear-complex-systems-chaos-just-with-the-application-i-want","text":"","title":"What I want to do in my MSc (also mention that I want to build the somewhat more \"general\" skillset; also mention stochastic calculus, dyn. sys (+ differential geometry, manifolds etc.) and nonlinear, complex systems, chaos, just with the application I want)"},{"location":"Projects/Thesis/Tsitsas%20Email/#what-resources-i-have-at-my-disposal-link-with-msc-asteris-the-friend","text":"","title":"What resources I have at my disposal -&gt; link with MSc (Asteris the friend)"},{"location":"Projects/Thesis/Tsitsas%20Email/#proposed-roadmap","text":"","title":"Proposed Roadmap"},{"location":"Projects/Website/Franklin.jl-notes/","text":"Franklin.jl Notes \u00b6 Modifications \u00b6 header/footer/general look: src/_css/ and src/_html_parts To customize single chunk, wrap in @@divname ... @@ , e.g. @@mybluebackground ... @@ Code \u00b6 @def hascode = true for syntax highlighting by highlight.js Can \"store\" code blocks: ```julia:./code/example using LinearAlgebra a = [ 1 , 2 , 3 , 3 , 4 , 5 , 2 , 2 ] @show dot ( a , a ) println ( dot ( a , a )) # hide ``` and then call with (lines with # hide are hidden): \\output{./code/example} To not have a code block execute multiple times because it's slow or from a different language, use \\input{julia}{/_assets/scripts/helloo.jl} which can still be written to file using scripts/generate_results.jl . The results can also be input: \\output{/_assets/scripts/helloo.jl} Using the latter approach with generate_results.jl is preferred because it ensures all of the code in the website works and that all results match the code which makes maintenance easier for plain text without highlighting, use ```plaintext ... ```. Without a language specifier, the default behavior is to go with Julia syntax highlighting more fancy highlihting: ```julia-repl (v1.4) pkg> add Franklin shell> blah julia> 1+1 (Sandbox) pkg> resolve ``` Symbols \u00b6 Can use HTML for symbols, e.g. &pi; or use VSCode autocompletion like so: \\pi[TAB] References \u00b6 [^name] Misc \u00b6 Force line break: // Relative links, e.g. [my article](/articles/my-article/) Table of contents: \\toc tag support: page with tag \"syntax\": [syntax](/tag/syntax/) $\\LaTeX$ \u00b6 \\newcommand \\eqref , \\cite , etc. Example: \\citep{bezanson17, noether15} , \\biblabel{bezanson17}{Bezanson et al. (2017)} **Bezanson**, **Edelman**, **Karpinski** and **Shah**, [Julia: a fresh approach to numerical computing](https://julialang.org/research/julia-fresh-approach-BEKS.pdf), SIAM review 2017. \\biblabel{noether15}{Noether (1915)} **Noether**, Ko\u0308rper und Systeme rationaler Funktionen, 1915. \\label{a cool label} with \\eqref{a cool label} , for math File header \u00b6 +++ title = \"Code blocks\" hascode = true date = Date(2019, 3, 22) rss = \"A short description of the page which would serve as **blurb** in a `RSS` feed; you can use basic markdown here but the whole description string must be a single line (not a multiline string). Like this one for instance. Keep in mind that styling is minimal in RSS so for instance don't expect maths or fancy styling to work; images should be ok though: ![](https://upload.wikimedia.org/wikipedia/en/3/32/Rick_and_Morty_opening_credits.jpeg)\" rss_title = \"More goodies\" rss_pubdate = Date(2019, 5, 1) tags = [\"syntax\", \"code\"] +++","title":"Franklin.jl Notes"},{"location":"Projects/Website/Franklin.jl-notes/#franklinjl-notes","text":"","title":"Franklin.jl Notes"},{"location":"Projects/Website/Franklin.jl-notes/#modifications","text":"header/footer/general look: src/_css/ and src/_html_parts To customize single chunk, wrap in @@divname ... @@ , e.g. @@mybluebackground ... @@","title":"Modifications"},{"location":"Projects/Website/Franklin.jl-notes/#code","text":"@def hascode = true for syntax highlighting by highlight.js Can \"store\" code blocks: ```julia:./code/example using LinearAlgebra a = [ 1 , 2 , 3 , 3 , 4 , 5 , 2 , 2 ] @show dot ( a , a ) println ( dot ( a , a )) # hide ``` and then call with (lines with # hide are hidden): \\output{./code/example} To not have a code block execute multiple times because it's slow or from a different language, use \\input{julia}{/_assets/scripts/helloo.jl} which can still be written to file using scripts/generate_results.jl . The results can also be input: \\output{/_assets/scripts/helloo.jl} Using the latter approach with generate_results.jl is preferred because it ensures all of the code in the website works and that all results match the code which makes maintenance easier for plain text without highlighting, use ```plaintext ... ```. Without a language specifier, the default behavior is to go with Julia syntax highlighting more fancy highlihting: ```julia-repl (v1.4) pkg> add Franklin shell> blah julia> 1+1 (Sandbox) pkg> resolve ```","title":"Code"},{"location":"Projects/Website/Franklin.jl-notes/#symbols","text":"Can use HTML for symbols, e.g. &pi; or use VSCode autocompletion like so: \\pi[TAB]","title":"Symbols"},{"location":"Projects/Website/Franklin.jl-notes/#references","text":"[^name]","title":"References"},{"location":"Projects/Website/Franklin.jl-notes/#misc","text":"Force line break: // Relative links, e.g. [my article](/articles/my-article/) Table of contents: \\toc tag support: page with tag \"syntax\": [syntax](/tag/syntax/)","title":"Misc"},{"location":"Projects/Website/Franklin.jl-notes/#latex","text":"\\newcommand \\eqref , \\cite , etc. Example: \\citep{bezanson17, noether15} , \\biblabel{bezanson17}{Bezanson et al. (2017)} **Bezanson**, **Edelman**, **Karpinski** and **Shah**, [Julia: a fresh approach to numerical computing](https://julialang.org/research/julia-fresh-approach-BEKS.pdf), SIAM review 2017. \\biblabel{noether15}{Noether (1915)} **Noether**, Ko\u0308rper und Systeme rationaler Funktionen, 1915. \\label{a cool label} with \\eqref{a cool label} , for math","title":"$\\LaTeX$"},{"location":"Projects/Website/Franklin.jl-notes/#file-header","text":"+++ title = \"Code blocks\" hascode = true date = Date(2019, 3, 22) rss = \"A short description of the page which would serve as **blurb** in a `RSS` feed; you can use basic markdown here but the whole description string must be a single line (not a multiline string). Like this one for instance. Keep in mind that styling is minimal in RSS so for instance don't expect maths or fancy styling to work; images should be ok though: ![](https://upload.wikimedia.org/wikipedia/en/3/32/Rick_and_Morty_opening_credits.jpeg)\" rss_title = \"More goodies\" rss_pubdate = Date(2019, 5, 1) tags = [\"syntax\", \"code\"] +++","title":"File header"},{"location":"References/%40ajo-franklinRationalDesignMemory2007/","text":"Rational design of memory in eukaryotic cells \u00b6 (2007) - Caroline M. Ajo-Franklin, David A. Drubin, Julian A. Eskin, Elaine P.S. Gee, Dirk Landgraf, Ira Phillips, Pamela A. Silver \u00b6 Link :: DOI :: 10.1101/gad.1586107 Backlinks :: Tags :: #paper Cite Key :: @ajo-franklinRationalDesignMemory2007 Abstract \u00b6 The ability to logically engineer novel cellular functions promises a deeper understanding of biological systems. Here we demonstrate the rational design of cellular memory in yeast that employs autoregulatory transcriptional positive feedback. We built a set of transcriptional activators and quantitatively characterized their effects on gene expression in living cells. Modeling in conjunction with the quantitative characterization of the activator-promoter pairs accurately predicts the behavior of the memory network. This study demonstrates the power of taking advantage of components with measured quantitative parameters to specify eukaryotic regulatory networks with desired properties. Notes \u00b6 Annotations (10/4/2022, 9:26:45 PM) \u201cThe ability to logically engineer novel cellular functions promises a deeper understanding of biological systems.\u201d (Ajo-Franklin et al., 2007, p. 2271)","title":"Rational design of memory in eukaryotic cells"},{"location":"References/%40ajo-franklinRationalDesignMemory2007/#rational-design-of-memory-in-eukaryotic-cells","text":"","title":"Rational design of memory in eukaryotic cells"},{"location":"References/%40ajo-franklinRationalDesignMemory2007/#2007-caroline-m-ajo-franklin-david-a-drubin-julian-a-eskin-elaine-ps-gee-dirk-landgraf-ira-phillips-pamela-a-silver","text":"Link :: DOI :: 10.1101/gad.1586107 Backlinks :: Tags :: #paper Cite Key :: @ajo-franklinRationalDesignMemory2007","title":"(2007) - Caroline M. Ajo-Franklin, David A. Drubin, Julian A. Eskin, Elaine P.S. Gee, Dirk Landgraf, Ira Phillips, Pamela A. Silver"},{"location":"References/%40ajo-franklinRationalDesignMemory2007/#abstract","text":"The ability to logically engineer novel cellular functions promises a deeper understanding of biological systems. Here we demonstrate the rational design of cellular memory in yeast that employs autoregulatory transcriptional positive feedback. We built a set of transcriptional activators and quantitatively characterized their effects on gene expression in living cells. Modeling in conjunction with the quantitative characterization of the activator-promoter pairs accurately predicts the behavior of the memory network. This study demonstrates the power of taking advantage of components with measured quantitative parameters to specify eukaryotic regulatory networks with desired properties.","title":"Abstract"},{"location":"References/%40ajo-franklinRationalDesignMemory2007/#notes","text":"","title":"Notes"},{"location":"References/%40alonIntroductionSystemsBiology2019/","text":"An introduction to systems biology: Design principles of biological circuits \u00b6 (2019) - Uri Alon \u00b6 Link :: DOI :: Backlinks :: Tags :: #paper Cite Key :: @alonIntroductionSystemsBiology2019 Abstract \u00b6 Notes \u00b6 Part one: Network motifs \u2013 Transcription networks basic concepts \u2013 Autoregulation \u2013 The feed forward loop \u2013 Temporal programs and the global structure of transcription networks \u2013 Positive feedback, bistability and memory \u2013 How to build a biological oscillator \u2013 Robustness \u2013 Kinetic proofreading and conformational proofreading \u2013 Part two: Robust signalling \u2013 Chemotaxis \u2013 Fold-change detection \u2013 Dynamical compensation and mutal resistance in tissues \u2013 Robust spatial patterning in development \u2013 Part three: Optimality \u2013 Optimal gene circuit design \u2013 Multi-objective optimality in biology \u2013 Modularity","title":"An introduction to systems biology: Design principles of biological circuits"},{"location":"References/%40alonIntroductionSystemsBiology2019/#an-introduction-to-systems-biology-design-principles-of-biological-circuits","text":"","title":"An introduction to systems biology: Design principles of biological circuits"},{"location":"References/%40alonIntroductionSystemsBiology2019/#2019-uri-alon","text":"Link :: DOI :: Backlinks :: Tags :: #paper Cite Key :: @alonIntroductionSystemsBiology2019","title":"(2019) - Uri Alon"},{"location":"References/%40alonIntroductionSystemsBiology2019/#abstract","text":"","title":"Abstract"},{"location":"References/%40alonIntroductionSystemsBiology2019/#notes","text":"Part one: Network motifs \u2013 Transcription networks basic concepts \u2013 Autoregulation \u2013 The feed forward loop \u2013 Temporal programs and the global structure of transcription networks \u2013 Positive feedback, bistability and memory \u2013 How to build a biological oscillator \u2013 Robustness \u2013 Kinetic proofreading and conformational proofreading \u2013 Part two: Robust signalling \u2013 Chemotaxis \u2013 Fold-change detection \u2013 Dynamical compensation and mutal resistance in tissues \u2013 Robust spatial patterning in development \u2013 Part three: Optimality \u2013 Optimal gene circuit design \u2013 Multi-objective optimality in biology \u2013 Modularity","title":"Notes"},{"location":"References/%40mattssonBoundaryOptimizedDiagonalnorm2018/","text":"Boundary optimized diagonal-norm SBP operators \u00b6 (2018) - Ken Mattsson, Martin Almquist, Edwin van der Weide \u00b6 Link :: DOI :: 10.1016/j.jcp.2018.06.010 Backlinks :: Tags :: #paper Cite Key :: @mattssonBoundaryOptimizedDiagonalnorm2018 Abstract \u00b6 Notes \u00b6","title":"Boundary optimized diagonal-norm SBP operators"},{"location":"References/%40mattssonBoundaryOptimizedDiagonalnorm2018/#boundary-optimized-diagonal-norm-sbp-operators","text":"","title":"Boundary optimized diagonal-norm SBP operators"},{"location":"References/%40mattssonBoundaryOptimizedDiagonalnorm2018/#2018-ken-mattsson-martin-almquist-edwin-van-der-weide","text":"Link :: DOI :: 10.1016/j.jcp.2018.06.010 Backlinks :: Tags :: #paper Cite Key :: @mattssonBoundaryOptimizedDiagonalnorm2018","title":"(2018) - Ken Mattsson, Martin Almquist, Edwin van der Weide"},{"location":"References/%40mattssonBoundaryOptimizedDiagonalnorm2018/#abstract","text":"","title":"Abstract"},{"location":"References/%40mattssonBoundaryOptimizedDiagonalnorm2018/#notes","text":"","title":"Notes"},{"location":"References/%40odiliStochasticProcessTutorial2022/","text":"Stochastic process and tutorial of the African buffalo optimization \u00b6 (2022) - Julius Beneoluchi Odili, A. Noraziah, Basem Alkazemi, M. Zarina \u00b6 Link :: DOI :: 10.1038/s41598-022-22242-9 Backlinks :: Tags :: #paper Cite Key :: @odiliStochasticProcessTutorial2022 Abstract \u00b6 Abstract This paper presents the data description of the African buffalo optimization algorithm (ABO). ABO is a recently-designed optimization algorithm that is inspired by the migrant behaviour of African buffalos in the vast African landscape. Organizing their large herds that could be over a thousand buffalos using just two principal sounds, the /maaa/ and the /waaa/ calls present a good foundation for the development of an optimization algorithm. Since elaborate descriptions of the manual workings of optimization algorithms are rare in literature, this paper aims at solving this problem, hence it is our main contribution. It is our belief that elaborate manual description of the workings of optimization algorithms make it user-friendly and encourage reproducibility of the experimental procedures performed using this algorithm. Again, our ability to describe the algorithm's basic flow, stochastic and data generation processes in a language so simple that any non-expert can appreciate and use as well as the practical implementation of the popular benchmark Rosenbrock and Shekel Foxhole functions with the novel algorithm will assist the research community in benefiting maximally from the contributions of this novel algorithm. Finally, benchmarking the good experimental output of the ABO with those of the popular, highly effective and efficient Cuckoo Search and Flower Pollination Algorithm underscores the ABO as a worthy contribution to the existing body of population-based optimization algorithms Notes \u00b6","title":"Stochastic process and tutorial of the African buffalo optimization"},{"location":"References/%40odiliStochasticProcessTutorial2022/#stochastic-process-and-tutorial-of-the-african-buffalo-optimization","text":"","title":"Stochastic process and tutorial of the African buffalo optimization"},{"location":"References/%40odiliStochasticProcessTutorial2022/#2022-julius-beneoluchi-odili-a-noraziah-basem-alkazemi-m-zarina","text":"Link :: DOI :: 10.1038/s41598-022-22242-9 Backlinks :: Tags :: #paper Cite Key :: @odiliStochasticProcessTutorial2022","title":"(2022) - Julius Beneoluchi Odili, A. Noraziah, Basem Alkazemi, M. Zarina"},{"location":"References/%40odiliStochasticProcessTutorial2022/#abstract","text":"Abstract This paper presents the data description of the African buffalo optimization algorithm (ABO). ABO is a recently-designed optimization algorithm that is inspired by the migrant behaviour of African buffalos in the vast African landscape. Organizing their large herds that could be over a thousand buffalos using just two principal sounds, the /maaa/ and the /waaa/ calls present a good foundation for the development of an optimization algorithm. Since elaborate descriptions of the manual workings of optimization algorithms are rare in literature, this paper aims at solving this problem, hence it is our main contribution. It is our belief that elaborate manual description of the workings of optimization algorithms make it user-friendly and encourage reproducibility of the experimental procedures performed using this algorithm. Again, our ability to describe the algorithm's basic flow, stochastic and data generation processes in a language so simple that any non-expert can appreciate and use as well as the practical implementation of the popular benchmark Rosenbrock and Shekel Foxhole functions with the novel algorithm will assist the research community in benefiting maximally from the contributions of this novel algorithm. Finally, benchmarking the good experimental output of the ABO with those of the popular, highly effective and efficient Cuckoo Search and Flower Pollination Algorithm underscores the ABO as a worthy contribution to the existing body of population-based optimization algorithms","title":"Abstract"},{"location":"References/%40odiliStochasticProcessTutorial2022/#notes","text":"","title":"Notes"}]}